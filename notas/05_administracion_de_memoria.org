#+SETUPFILE: ../setup_notas.org
#+TITLE: Administración de memoria

* Funciones y operaciones
# <<MEM>>
El único espacio de almacenamiento que el procesador puede utilizar
directamente, más allá de los registros (que si bien le son internos y
sumamente rápidos, son de capacidad demasiado limitada) es la memoria
física. Todas las arquitecturas de procesador tienen instrucciones
para interactuar con la memoria, pero ninguna lo tiene para hacerlo
con medios /persistentes/ de almacenamiento, como las unidades de
disco. Cabe mencionar que cuando se encuentre en un texto
referencia al /almacenamiento primario/ siempre se referirá a la
memoria, mientras que el /almacenamiento secundario/ se refiere a los
discos u otros medios de almacenamiento persistente.

Todos los programas a ejecutar deben cargarse a la memoria del sistema
antes de ser utilizados. En este capítulo se mostrará cómo el sistema
operativo administra la memoria para permitir que varios procesos la
compartan — esta tarea debe preverse desde el proceso de compilación
de los programas (en particular, la fase de /ligado/). Hoy en día,
además, casi todos los sistemas operativos emplean implementaciones
que requieren de hardware especializado: la /Unidad de Manejo de
Memoria/ (*mmu*, por sus siglas en inglés). En el transcurso de este
capítulo se describirá cómo se manejaban los sistemas multitarea antes
de la universalización de las *mmu*, y qué papel juegan hoy en día.

En esta primer sección se presentarán algunos conceptos base que se
emplearán en las secciones subsecuentes.

** Espacio de direccionamiento

La memoria está estructurada como un arreglo direccionable de
bytes. Esto es, al solicitar el contenido de una dirección
específica de memoria, el hardware entregará un byte (8 bits), y
no menos. Si se requiere hacer una operación sobre /bits/ específicos,
se deberá solicitar y almacenar bytes enteros. En algunas
arquitecturas, el /tamaño de palabra/ es mayor —por ejemplo, los
procesadores Alpha incurrían en /fallas de alineación/ si se
solicitaba una dirección de memoria no alineada a 64 bits, y toda
llamada a direcciones /mal alineadas/ tenía que ser /atrapada/ por el
sistema operativo, re-alineada, y entregada.

Un procesador que soporta un /espacio de direccionamiento/ de 16 bits
puede referirse /directamente/ a hasta $2^{16}$ bytes, esto es, hasta
$65~536$ bytes (64 *kb*). Estos procesadores fueron comunes en las
décadas de 1970 y 1980 — los más conocidos incluyen al Intel 8080 y
8085, Zilog *z80*, *mos* 6502 y 6510, y Motorola 6800. Hay que
recalcar que estos procesadores son reconocidos como procesadores de
/8 bits/, pero con /espacio de direccionamiento/ de 16 bits. El
procesador empleado en las primeras *pc*, el Intel 8086, manejaba un
direccionamiento de 20 bits (hasta $1~024$ *kb*), pero al ser una
arquitectura /real/ de 16 bits requería del empleo de /segmentación/
para alcanzar toda su memoria.

Con la llegada de la era de las /computadoras personales/, diversos
fabricantes introdujeron, a mediados de los años 1980, los procesadores
con arquitectura de 32 bits. Por ejemplo, *x86-32* de Intel tiene su
inicio oficial con el procesador 80386 (o simplemente 386).
Este procesador podía referenciar desde el punto de vista teórico
hasta $2^{32}$ bytes (4 *gb*) de *ram*. No obstante, debido a las
limitaciones tecnológicas (y tal vez estratégicas) para producir
memorias con esta capacidad, tomó más de 20 años para que las
memorias ampliamente disponibles alcanzaran dicha capacidad.

Hoy en día, los procesadores dominantes son de 32 o 64 bits. En el caso de
los procesadores de 32 bits, sus registros pueden referenciar hasta
$4~294~967~296$ bytes (4 *gb*) de *ram*, que está ya dentro de los
parámetros de lo esperable hoy en día. Una arquitectura de
32 bits sin extensiones adicionales no puede emplear una memoria
de mayor capacidad. No obstante, a través de un mecanismo llamado *pae*
(extensión de direcciónes físicas, /Physical Address Extension/)
permite extender esto a rangos de hasta $2^{52}$ bytes a cambio de
un nivel más de indirección.

Un procesador de 64 bits podría direccionar hasta
$18~446~744~073~709~551~616$ bytes (16 Exabytes). Los procesadores
comercialmente hoy en día no ofrecen esta capacidad de
direccionamiento principalmente por un criterio económico: al resultar
tan poco probable que exista un sistema con estas capacidades, los
chips actuales están limitados a entre $2^{40}$ y $2^{48}$ bits — 1 y
256 terabytes. Esta restricción debe seguir teniendo sentido económico
por muchos años aún.

** Hardware: la unidad de manejo de memoria (*mmu*)

Con la introducción de sistemas multitarea, es decir, donde se tienen
dos o más programas ejecutandose, apareció la necesidad de tener más
de un programa cargado en memoria. Esto conlleva que el sistema
operativo, contando con información de los programas a ejecutar, debe
resolver cómo ubicar los programas en la memoria física disponible.

A medida que los sistemas operativos y los programas crecieron en tamaño,
fue necesario /abstraer/ el espacio de almacenamiento para dar la ilusión
de contar con más memoria de la que está directamente disponible. Con
asistencia del hardware, es posible configurar un espacio lineal contiguo
para cada proceso -y para el mismo sistema operativo que se /proyecta/ a
memoria física y a un almacenamiento secundario.

Se explicará cómo la *mmu* cubre estas necesidades y qué mecanismos
emplea para lograrlo —y qué cuidados se deben observar, incluso como
programadores de aplicaciones en lenguajes de alto nivel, para aprovechar de la
mejor manera estas funciones (y evitar, por el contrario, que los
programas se vuelvan lentos por no manejar la memoria
correctamente).

La *mmu* es también la encargada de verificar que un proceso no tenga
acceso a leer o modificar los datos de otro —si el sistema operativo
tuviera que verificar cada una de las instrucciones ejecutadas por un
programa para evitar errores en el acceso a la memoria, la
penalización en velocidad sería demasiado severa.[fn:: Y de hecho está
demostrado que no puede garantizarse que una verificación estática sea
suficientemente exhaustiva]

Una primera aproximación a la protección de acceso se implementa
usando un /registro base/ y un /registro límite/: si la arquitectura
ofrece dos registros del procesador que sólo pueden ser modificados
por el sistema operativo (esto es, el hardware define la modificación
de dichos registros como una operación privilegiada que requiere estar
ejecutando en /modo supervisor/), la *mmu* puede comparar sin
penalidad /cada acceso a memoria/ para verificar que esté en el rango
permitido.

Por ejemplo, si a un proceso le fue asignado un espacio de memoria de
64 *kb* ($65~536$ bytes) a partir de la dirección $503~808$
(492 *kb*), el /registro base/ contendría $503~808$, y el /registro
límite/ $65~536$. Si hubiera una instrucción por parte de dicho
proceso que solicitara una dirección menor a $503~808$ o mayor a
$569~344$ (556 *kb*), la *mmu* lanzaría una excepción o /trampa/
interrumpiendo el procesamiento, e indicando al sistema operativo que
ocurrió una /violación de segmento/ (/segmentation fault/).[fn:: ¿Por
qué /de segmento/? Véase la sección \ref{MEM_segmentacion}.] El
sistema operativo entonces procedería a terminar la ejecución del
proceso, reclamando todos los recursos que tuviera asignados y
notificando a su usuario.

#+attr_latex: width=\textwidth
#+label: MEM_mem_base_a_limite
#+caption: Espacio de direcciones válidas para el proceso 3 definido por un registro base y uno límite.
[[./img/ditaa/mem_base_a_limite.png]]

** La memoria /caché/

Hay otro elemento en la actualidad que se asume como un hecho: la
memoria /caché/. Si bien su manejo es (casi) transparente para el
sistema operativo, es muy importante mantenerlo en mente.

Conforme el procesador avanza en la ejecución de las instrucciones
(aumentando el valor almacenado en el registro de conteo de
instrucción), se producen accesos a memoria. Por un lado, tiene que
buscar en memoria la siguiente instrucción a ejecutar. Por otro, estas
instrucciones pueden requerirle uno o más operadores adicionales que
deban ser leídos de la memoria. Por último, la instrucción puede
requerir guardar su resultado también en memoria.

Hace años esto no era un problema —la velocidad del procesador
estaba básicamente sincronizada con la del manejador de memoria, y el
flujo podía mantenerse básicamente estable. Pero conforme los
procesadores se fueron haciendo más rápidos, y conforme se ha
popularizado el procesamiento en paralelo, la tecnología de la memoria 
no ha progresado a la misma velocidad. La memoria de alta velocidad es
demasiado cara, e incluso las distancias de unos pocos centímetros se
convierten en obstáculos insalvables por la velocidad máxima de los
electrones viajando por pistas conductoras.

Cuando el procesador solicita el contenido de una dirección de memoria
y esta no está aún disponible, /tiene que detener su ejecución/
(/stall/) hasta que los datos estén disponibles. El *cpu* no puede, a
diferencia del sistema operativo, "congelar" todo y guardar el estado
para atender otro proceso: para el procesador, la lista de
instrucciones a ejecutar es estrictamente secuencial, y todo tiempo
que requiere esperar una transferencia de datos es tiempo perdido.

La respuesta para reducir esa espera es la /memoria caché/. Esta es
una memoria de alta velocidad, situada /entre/ la memoria principal y el
procesador propiamente, que guarda copias de las /páginas/ que van
siendo accesadas, partiendo del principio de la /localidad de
referencia/:

- Localidad temporal :: Es probable que un recurso que fue empleado
     recientemente vuelva a emplearse en un futuro cercano.
- Localidad espacial :: La probabilidad de que un recurso /aún no
     requerido/ sea accesado es mucho mayor si fue requerido algún
     recurso cercano.
- Localidad secuencial :: Un recurso, y muy particularmente la
     memoria, tiende a ser requerido de forma secuencial.

#+label: MEM_localidad_de_referencia
#+caption: Patrones de acceso a memoria, demostrando la localidad espacial/temporal (Silberschatz, Galvin y Gagné 2010: 350).
#+attr_html: width:"330"
#+attr_latex: width=0.7\textwidth
[[./img/localidad_de_referencia.png]]

Aplicando el concepto de localidad de referencia, cuando el procesador solicita
al hardware determinada dirección de memoria, el hardware no sólo
transfiere a la memoria caché el byte o palabra solicitado, sino que
transfiere un bloque o /página/ completo.

Cabe mencionar que hoy en día (particularmente desde que se detuvo la
/guerra de los Megahertz/), parte importante del diferencial en precios
de los procesadores líderes en el mercado es la cantidad de memoria caché de
primero y segundo nivel con que cuentan.

** El espacio en memoria de un proceso
# <<MEM_espacio_en_memoria>>

Cuando un sistema operativo inicia un proceso, no se limita a volcar
el archivo ejecutable a memoria, sino que tiene que proporcionar la
estructura para que éste vaya guardando la información de estado
relativa a su ejecución.

- Sección (o segmento) de texto :: Es el nombre que recibe la imagen en memoria de
     las instrucciones a ser ejecutadas. Usualmente, la sección de
     texto ocupa las direcciones /más bajas/ del espacio en memoria.

- Sección de datos :: Espacio fijo preasignado para las variables
     globales y datos inicializados (como las cadena de caracteres por
     ejemplo). Este espacio es fijado en tiempo de compilación, y no
     puede cambiar (aunque los datos que cargados allí sí cambian en
     el tiempo de vida del proceso).

- Espacio de /libres/ ::  Espacio de memoria que se emplea
     para la asignación dinámica de memoria /durante la ejecución/ del
     proceso. Este espacio se ubica por encima de la sección de datos,
     y /crece hacia arriba/. Este espacio es conocido en inglés como
     el /Heap/.

     Cuando el programa es escrito en lenguajes que requieren /manejo
     dinámico manual de la memoria/ (como C), esta área es la que se maneja
     mediante las llamadas de la familia de =malloc= y =free=. En
     lenguajes con gestión automática, esta área es monitoreada por
     los /recolectores de basura/.

- Pila de llamadas :: Consiste en un espacio de memoria que se usa
     para almacenar la secuencia de funciones que han sido llamadas
     dentro del proceso, con sus parámetros, direcciones de /retorno/,
     variables locales, etc. La pila ocupa la parte /más alta/ del
     espacio en memoria, y /crece hacia abajo/. En inglés, la pila de
     llamadas es denominada /Stack/.

#+attr_latex: width=0.5\textwidth
#+label: MEM_proceso_en_memoria
#+caption: Regiones de la memoria para un proceso.
[[./img/ditaa/proceso_en_memoria.png]]


** Resolución de direcciones

Un programa compilado no emplea nombres simbólicos para las variables
o funciones que llama;[fn:: Cuando se hace /ligado dinámico/ a
bibliotecas externas sí se mantiene la referencia por nombre, pero
para los propósitos de esta sección, se habla de las referencias
internas únicamente] el compilador, al convertir el programa a
lenguaje máquina, las sustituye por la dirección en memoria donde se
encuentra la variable o la función.[fn:: De hecho, una vez que el programa
se pasa a lenguaje de máquina, no hay diferencia real entre la
dirección que ocupa una variable
o código ejecutable. La diferencia se establece por el uso que se dé a la
referencia de memoria. En la sección \ref{MEM_buffer_overflow} se
abordará un ejemplo de cómo esto puede tener importantes consecuencias.]

Ahora bien, en los sistemas actuales, los procesos requieren coexistir
con otros, para lo cual las direcciones indicadas en el /texto/ del
programa pueden requerir ser traducidas al lugar /relativo al sitio de
inicio del proceso en memoria/ —esto es, las direcciones son
/resueltas/ o traducidas.  Hay diferentes estrategias de resolución,
que se pueden clasificar a grandes rasgos[fn:: Esta explicación
simplifica muchos detalles; para el lector interesado en profundizar
en este tema, se recomienda el libro /Linkers and Loaders/ (/Ligadores
y cargadores/) \parencite{Levine1999}, disponible en línea.] en:

- En tiempo de compilación :: El texto del programa tiene la dirección
     /absoluta/ de las variables y funciones. Esto era muy común en las
     computadoras previas al multiprocesamiento. En la arquitectura
     compatible con *pc*, el formato ejecutable =.COM= es un volcado de
     memoria directo de un archivo objeto con las direcciones
     indicadas de forma absoluta. Esto puede verse hoy
     principalmente en sistemas embebidos o de función específica.

- En tiempo de carga :: Al cargarse a memoria el programa y antes de
     iniciar su ejecución, el /cargador/ (componente del sistema
     operativo) actualiza las referencias a memoria dentro del texto
     para que apunten al lugar correcto —claro está, esto depende de
     que el compilador indique dónde están todas las referencias a las
     variables y las funciones.

- En tiempo de ejecución :: El programa nunca hace referencia a una
     ubicación absoluta de memoria, sino que lo hace siempre relativo
     a una /base/ y un /desplazamiento/ (offset). Esto permite que el
     proceso sea incluso reubicado en la memoria mientras está siendo
     ejecutado sin tener que sufrir cambios, pero requiere de hardware
     específico (como una *mmu*).

Esto es, los nombres simbólicos (por ejemplo, una variable llamada
=contador=) para ser traducidos ya sea a ubicaciones en la memoria,
pueden resolverse en tiempo de compilación (y quedar plasmada en el
programa en disco con una ubicación explícita y definitiva:
$510~200)$, en tiempo de carga (sería guardada en el programa en disco
como /inicio/ + $5~986$ bytes, y el proceso de carga incluiría
sustituirla por la dirección resuelta a la suma del registro base,
$504~214$, y el /desplazamiento/, $5~986$, esto es, $510~200)$.

Por último, para emplear la resolución en tiempo de ejecución, se
mantiene en las instrucciones a ser ejecutadas por el proceso la
etiqueta relativa al módulo actual, /inicio/ + $5~986$ bytes, y es
resuelta cada vez que sea requerido.

#+attr_latex: width=0.4\textwidth
#+label: MEM_tipo_resol_direcc
#+caption: Proceso de compilación y carga de un programa, indicando el tipo de resolución de direcciones (Silberschatz, Galvin y Gagné 2010: 281).
[[./img/dot/tipo_resol_direcc.png]]

* Asignación de memoria contigua

En los sistemas de ejecución en lotes, así como en las primeras
computadoras personales, sólo un programa se ejecutaba a la vez. Por
lo que, más allá de la carga del programa y la satisfacción de alguna
eventual llamada al sistema solicitando recursos, el sistema operativo
no tenía que ocuparse de la asignación de memoria.

Al nacer los primeros sistemas operativos multitarea, se hizo
necesario resolver cómo asignar el espacio en memoria a diferentes
procesos.

** Partición de la memoria

La primer respuesta, claro está, es la más sencilla: asignar
a cada programa a ser ejecutado un bloque /contiguo/ de memoria de un
tamaño fijo. En tanto los programas permitieran la resolución de
direcciones en tiempo de carga o de ejecución, podrían emplear este
esquema.

El sistema operativo emplearía una región específica de la memoria del
sistema (típicamente la /región baja/ —desde la dirección en memoria
0x00000000 hacia arriba), y una vez terminado el espacio necesario
para el núcleo y sus estructuras, el sistema asigna espacio a cada uno
de los procesos. Si la arquitectura en cuestión permite limitar los
segmentos disponibles a cada uno de los procesos (por ejemplo, con los
registros /base/ y /límite/ discutidos anteriormente), esto sería
suficiente para alojar en memoria varios procesos y evitar que
interfieran entre sí.

Desde la perspectiva del sistema operativo, cada uno de los espacios
asignados a un proceso es una /partición/. Cuando el sistema operativo
inicia, toda la memoria disponible es vista como un sólo bloque, y
conforme se van ejecutando procesos, este bloque va siendo subdividido
para satisfacer sus requisitos. Al cargar un programa el sistema
operativo calcula cuánta memoria va a requerir a lo largo de su vida prevista.
Esto incluye el espacio requerido para la asignación dinámica de memoria
con la familia de funciones =malloc= y =free=.

*** Fragmentación
# <<MEM_fragmentacion>>

Es un fenómeno que se manifiesta a medida que los procesos terminan su
ejecución, y el sistema operativo libera la memoria asignada a cada uno de
ellos. Si los procesos se encontraban en regiones de memoria, apartadas
entre sí, comienzan a aparecer regiones de memoria disponible, /interrumpidas/
por regiones de memoria usada por los procesos que aún se encuentran activos.

Si la computadora no tiene hardware específico que permita que los
procesos resuelvan sus direcciones en tiempo de ejecución, el sistema
operativo no puede reasignar los bloques existentes, y aunque pudiera
hacerlo, mover un proceso entero en memoria puede resultar una
operación costosa en tiempo de procesamiento.

Al crear un nuevo proceso, el sistema operativo tiene tres
estrategias según las cuales podría asignarle uno de los bloques
disponibles:

- Primer ajuste :: El sistema toma el primer bloque con el tamaño 
                   suficiente para alojar el nuevo proceso. Este
                   es el mecanismo más simple de implementar y el de 
                   más rápida ejecución. No obstante, esta estrategia
                   puede causar el desperdicio de memoria, si el bloque
		   no es exactamente del tamaño requerido.

- Mejor ajuste :: El sistema busca entre todos los bloques disponibles
                  cuál es el que mejor se ajusta al tamaño requerido
                  por el nuevo proceso. Esto implica la revisión
                  completa de la lista de bloques, pero permite que los
                  bloques remanentes, una vez que se ubicó al nuevo
                  proceso, sean tan pequeños como sea posible (esto
                  es, que haya de hecho un /mejor ajuste/).

- Peor ajuste :: El sistema busca cuál es el bloque más grande
                 disponible, y se lo asigna al nuevo
                 proceso. Empleando una estructura de datos como un
                 /montículo/, esta operación puede ser incluso más
                 rápida que la de primer ajuste. Con este mecanismo
                 se busca que los bloques que queden después de
                 otorgarlos a un proceso sean tan grandes como sea
                 posible, de cierto modo balanceando su tamaño.

La /fragmentación externa/ se produce cuando hay muchos bloques libres
entre bloques asignados a procesos; la /fragmentación interna/ se
refiere a la cantidad de memoria dentro de un bloque que nunca se
usará —por ejemplo, si el sistema operativo maneja /bloques/ de 512
bytes y un proceso requiere sólo 768 bytes para su ejecución, el
sistema le entregará dos bloques ($1~024$ bytes), con lo cual desperdicia
256 bytes. En el peor de los casos, con un bloque de $n$ bytes, un
proceso podría solicitar $kn+1$ bytes de memoria, desperdiciando por
fragmentación interna $n-1$ bytes.

Según análisis estadísticos \parencite[289]{Silberschatz2010}, por
cada /N/ bloques asignados, se perderán del orden de /0.5N/ bloques
por fragmentación interna y externa.

*** Compactación

Un problema importante que va surgiendo como resultado de esta
fragmentación es que el espacio total libre de memoria puede ser mucho
mayor que lo que requiere un nuevo proceso, pero al estar
/fragmentada/ en muchos bloques, éste no encontrará una partición
contigua donde ser cargado.

Si los procesos emplean resolución de direcciones en tiempo de
ejecución, cuando el sistema operativo comience a detectar un alto
índice de fragmentación, puede lanzar una operación de /compresión/ o
/compactación/. Esta operación consiste en mover los contenidos en
memoria de los bloques asignados para que ocupen espacios contiguos,
permitiendo unificar varios bloques libres contiguos en uno solo.

#+attr_latex: width=0.6\textwidth
#+label: MEM_compactacion_memoria
#+caption: Compactación de la memoria de procesos en ejecución.
[[./img/ditaa/compactacion_memoria.png]]

La compactación tiene un costo alto —involucra mover prácticamente la
totalidad de la memoria (probablemente más de una vez por bloque).

*** Intercambio (/swap/) con el almacenamiento secundario
# <<MEM_swap>>

Siguiendo de cierto modo la lógica requerida por la compactación
se encuentran los sistemas que utilizan /intercambio/ (/swap/) entre la
memoria primaria y secundaria. En éstos, el sistema operativo
puede /comprometer/ más espacio de memoria del que tiene físicamente
disponible. Cuando la memoria se acaba, el
sistema suspende un proceso (usualmente un proceso "bloqueado")
y almacena una copia de su imagen en memoria en almacenamiento secundario
para luego poder restaurarlo.

Hay algunas restricciones que observar previo a suspender un proceso.
Por ejemplo, se debe considerar si el proceso tiene pendiente alguna
operación de entrada/salida, en la cual el resultado se deberá copiar
en su espacio de memoria. Si el proceso resultara suspendido
(retirándolo de la memoria principal), el sistema operativo no tendría
dónde continuar almacenando estos datos conforme llegaran.
Una estrategia ante esta situación podría ser
que todas las operaciones se realicen únicamente a /buffers/ (regiones de
memoria de almacenamiento temporal) en el espacio del sistema operativo,
y éste las transfiera el contenido del buffer al espacio de memoria del
proceso suspendido una vez que la operación finalice.

Esta técnica se popularizó en los sistemas de escritorio hacia finales
de los 1980 y principios de los 1990, en que las computadoras
personales tenían típicamente entre 1 y 8 *mb* de memoria.

Se debe considerar que las unidades de disco son del orden de decenas de miles
de veces más lentas que la memoria, por lo que este proceso resulta
muy caro. Por ejemplo, si la imagen en memoria de un proceso es de
100 *mb*, bastante conservador hoy en día, y la tasa de transferencia
sostenida al disco de 50
#+latex: \textsc{mb}/s,
#+html: <b>mb</b>/s,
intercambiar un proceso al disco toma
dos segundos. Cargarlo de vuelta a la memoria toma otros dos segundos
— y a esto debe sumarse el tiempo de posicionamiento de la cabeza de
lectura/escritura, especialmente si el espacio a emplear no es
secuencial y contiguo. Resulta obvio por qué esta técnica ha caído en
desuso conforme aumenta el tamaño de los procesos.

* Segmentación
# <<MEM_segmentacion>>

Al desarrollar un programa en un lenguaje de alto nivel, el programador
usualmente no se preocupa por la ubicación en la memoria física de los
diferentes elementos que lo componen. Esto se debe a que
en estos lenguajes las variables y funciones son referenciadas por sus
/nombres/, no por su ubicación[fn:: Al programar en lenguaje C por ejemplo, un
programador puede trabajar a este mismo nivel de abstracción, puede
referirse directamente a las ubicaciones en memoria de sus datos
empleando /aritmética de apuntadores/.]. No obstante, cuando se compila el
programa para una arquitectura que soporte segmentación, el compilador
ubicará a cada una de las secciones presentadas en la sección
\ref{MEM_espacio_en_memoria} en un segmento diferente.

Esto permite activar los mecanismos que evitan la escritura accidental de las 
secciones de memoria del proceso que no se deberían modificar 
(aquellas que contienen  código o de sólo lectura), y permitir la escritura de aquellas 
que sí (en las cuales se  encuentran las variables globales, 
la pila o /stack/ y el espacio de asignación dinámica o /heap/).

Así, los elementos que conforman un programa se organizan en /secciones/:
 una sección contiene el espacio para las  variables globales, 
otra sección contiene el código compilado, otra sección 
contiene la /tabla de símbolos/, etc.

Luego, cuando el sistema operativo crea un proceso a partir del programa,
debe organizar el contenido del archivo ejecutable en memoria. Para ello
carga en memoria algunas secciones del archivo ejecutable (como mínimo
la sección para las variables globales y la sección de código) y puede
configurar otras secciones como la pila o la sección de libres. 
Para garantizar la protección de cada una de estas secciones en la
memoria del proceso, el sistema puede definir que cada /sección/ del 
programa se encuentra en un /segmento/ diferente, con diferentes 
tipos de acceso.


#+label: MEM_segmentacion_de_memoria
#+caption: Ejemplo de segmentación.
#+attr_latex: width=\textwidth
[[./img/dot/segmentacion_de_memoria.png]]

La segmentación es un concepto que se aplica directamente a la
arquitectura del procesador. Permite separar las regiones de la
memoria /lineal/ en /segmentos/, cada uno de los cuales puede tener
diferentes permisos de acceso, como se explicará en la siguiente
sección. La segmentación también ayuda a incrementar la /modularidad/
de un programa: es muy común que las bibliotecas /ligadas
dinámicamente/ estén representadas en segmentos independientes.

Un código compilado para procesadores que implementen segmentación
siempre generará referencias a la memoria en un espacio /segmentado/. Este 
tipo de referencias se denominan /direcciones lógicas/ y están formadas por un
/selector/ de segmento y un /desplazamiento/ dentro del segmento. 
Para interpretar esta dirección, la *mmu* debe tomar el selector, y usando 
alguna estructura de datos, 
obtiene la dirección base, el tamaño del segmento y sus atributos
de protección. Luego, aplicando el mecanismo explicado en secciones
anteriores, toma la dirección base del segmento y le suma el desplazamiento
para obtener una /dirección lineal física/. 

La traducción de una dirección lógica a una dirección lineal puede fallar
por diferentes razones: si el segmento no se encuentra en memoria, ocurrirá
una /excepción/ del tipo /segmento no presente/. Por otro lado, si el 
desplazamiento especificado es mayor al tamaño definido para el segmento,
ocurrirá una excepción del tipo /violación de segmento/.

** Permisos
# <<MEM_permisos>>

Una de las principales ventajas del uso de segmentación consiste en
permitir que cada uno de los segmentos tenga un distinto
/juego de permisos/ para el proceso en cuestión: el sistema operativo
puede indicar, por ejemplo, que el /segmento de texto/ (el código del
programa) sea de lectura y ejecución, mientras que las secciones de
datos, libres y pila (donde se almacena y trabaja la información misma
del programa) serán de lectura y escritura, pero la ejecución estará
prohibida.[fn:: Si bien este es el manejo clásico, no es una regla
inquebrantable: el código /automodificable/ conlleva importantes
riesgos de seguridad, pero bajo ciertos supuestos, el sistema debe
permitir su ejecución. Además, muchos lenguajes de programación
permiten la /metaprogramación/, que requiere la ejecución de código
construido en tiempo de ejecución.]

De este modo, se puede evitar que un error en
la programación resulte en que datos proporcionados por el usuario o
por el entorno modifiquen el código que está siendo ejecutado.[fn::
Sin embargo, incluso bajo este esquema, dado que la /pila de llamadas/
(/stack/) debe mantenerse como escribible, es común encontrar ataques
que permiten modificar la /dirección de retorno/ de una subrutina,
como será descrito en la sección \ref{MEM_buffer_overflow}.] Es
más, dado que el acceso de /ejecución/ está limitado a sólo
los segmentos cargados del disco por el sistema operativo, un atacante
no podrá introducir código ejecutable tan fácilmente —tendría que
cargarlo como un segmento adicional con los permisos correspondientes.

La segmentación también permite distinguir /niveles/ de acceso a la
memoria: para que un proceso pueda efectuar /llamadas al sistema/,
debe tener acceso a determinadas estructuras compartidas del
núcleo. Claro está, al ser memoria privilegiada, su acceso requiere
que el procesador esté ejecutando en /modo supervisor/.

** Intercambio parcial
# <<MEM_intercambio_parcial>>

Un uso muy común de la segmentación, particularmente en los sistemas
de los 1980, era el de permitir que sólo /ciertas regiones/ de un
programa sean intercambiadas al disco: si un programa está compuesto
por porciones de código que nunca se ejecutarán aproximadamente al
mismo tiempo en sucesión, puede separar su texto (e incluso los datos
correspondientes) en diferentes segmentos.

A lo largo de la ejecución del programa, algunos de sus segmentos
pueden no emplearse por largos periodos. Éstos pueden ser enviados al
/espacio de intercambio/ (/swap/) ya sea a solicitud del proceso o por
iniciativa del sistema operativo.

*** Rendimiento

En la sección \ref{MEM_swap}, donde se presenta el concepto de
intercambio, se explicó que intercambiar un proceso completo
resultaba demasiado caro. Cuando se tiene de un espacio de memoria
segmentado y, muy particularmente, cuando se usan bibliotecas de
carga dinámica, la sobrecarga es mucho menor.

En primer término, se puede hablar de la cantidad de información a
intercambiar: en un sistema que sólo maneja regiones contiguas de
memoria, intercambiar un proceso significa mover toda su información
al disco. En un sistema con segmentación, puede enviarse a disco
cada uno de los segmentos por separado, según el sistema operativo
lo juzgue necesario. Podría /sacar/ de memoria a alguno de los segmentos,
eligiendo no necesariamente al que más /estorbe/ (esto es, el más grande),
sino el que más probablemente no se utilice: emplear el principio
de localidad de referencia para intercambiar al segmento /menos
recientemente utilizado/ (*lru*, /least recently used/).

Además de esto, si se tiene  un segmento de texto (sea el código
programa base o alguna de las bibliotecas) y su acceso es de sólo
lectura, una vez que éste fue copiado ya al disco, no hace
falta volver a hacerlo: se tiene la certeza de que no será modificado
por el proceso en ejecución, por lo que basta marcarlo como /no
presente/ en las tablas de segmentos en memoria para que cualquier
acceso ocasione que el sistema operativo lo traiga de disco.

Por otro lado, si la biblioteca en cuestión reside en disco (antes de
ser cargada) como una imagen directa de su representación en memoria,
al sistema operativo le bastará identificar el archivo en cuestión al
cargar el proceso; no hace falta siquiera cargarlo en la memoria
principal y guardarlo al área de intercambio, puede quedar referido
directamente al espacio en disco en que reside el archivo.

Claro está, el acceso a disco sigue siendo una fuerte penalización
cada vez que un segmento tiene que ser cargado del disco (sea del
sistema de archivos o del espacio de intercambio), pero este mecanismo
reduce dicha penalización, haciendo más atractiva la flexibilidad del
intercambio por segmentos.

** Ejemplificando

A modo de ejemplo, y conjuntando los conceptos presentados en esta
sección, si un proceso tuviera la siguiente tabla de segmentos:

| Segmento | Inicio   | Tamaño | Permisos | Presente |
|----------+----------+--------+----------+----------|
|        0 | $15~208$ |    160 | *rwx*    | sí       |
|        1 | $1~400$  |    100 | *r*      | sí       |
|        2 | 964      |     96 | *rx*     | sí       |
|        3 | -        |    184 | *w*      | no       |
|        4 | $10~000$ |    320 | *rwx*    | sí       |

En la columna de permisos, se indica con /R/ el permiso de lectura
(/Read/), /W/ el de escritura (/Write/), y /X/ el de ejecución
(/eXecute/).

Un segmento que ha sido enviado al espacio de intercambio (en este
caso, el 3), deja de estar presente en memoria y, por tanto, no tiene
ya dirección de inicio registrada.

El resultado de hacer referencia a las siguientes direcciones y
modos:

| Dirección | Tipo de | Dirección                               |
|    lógica | acceso  | física                                  |
|-----------+---------+-----------------------------------------|
|     0-100 | *r*     | $15~308$                                |
|      2-84 | *x*     | $1~048$                                 |
|      2-84 | *w*     | Atrapada: violación de seguridad        |
|     2-132 | *r*     | Atrapada: desplazamiento fuera de rango |
|      3-16 | *w*     | Atrapada: segmento faltante             |
|     3-132 | *r*     | Atrapada: segmento faltante;            |
|           |         | violación de seguridad                  |
|     4-128 | *x*     | $10~128$                                |
|      5-16 | *x*     | Atrapada: segmento inválido             |

Cuando se atrapa una situación de excepción, el sistema operativo debe
intervenir. Por ejemplo, la solicitud de un segmento inválido, de un
desplazamiento mayor al tamaño del segmento, o de un tipo de acceso
que no esté autorizado, típicamente llevan a la terminación del
proceso, en tanto que una de segmento faltante (indicando un segmento
que está en el espacio de intercambio) llevaría a la suspensión del
proceso, lectura del segmento de disco a memoria, y una vez que éste
estuviera listo, se permitiría la continuación de la ejecución.

En caso de haber más de una excepción, como se observa en la
solicitud de lectura de la dirección 3-132, el sistema debe reaccionar
primero a la /más severa/: si como resultado de esa solicitud iniciara
el proceso de carga del segmento, sólo para abortar la ejecución del
proceso al detectarse la violación de tipo de acceso, sería un
desperdicio injustificado de recursos.

* Paginación
# <<MEM_paginacion>>

La fragmentación externa y, por tanto, la necesidad de compactación
pueden evitarse por completo empleando la /paginación/. Ésta consiste
en que cada proceso está dividio en varios bloques
de tamaño fijo (más pequeños que los segmentos) llamados /páginas/, dejando
de requerir que la asignación sea de un área /contigua/ de
memoria. Claro está, esto requiere de mayor soporte por parte
del hardware, y mayor información relacionada a cada uno de los
procesos: no basta sólo con indicar dónde inicia y termina el
área de memoria de cada proceso, sino que se debe establecer un /mapeo/
entre la ubicación real (/física/) y la presentada a cada uno de los
procesos (/lógica/). La memoria se presentará a cada proceso como si
fuera de su uso exclusivo.

La memoria física se divide en una serie de /marcos/ (/frames/), todos
ellos del mismo tamaño, y el espacio para cada proceso se divide en una
serie de páginas (/pages/), del mismo tamaño que los marcos. La *mmu*
se encarga del mapeo entre páginas y marcos mediante /tablas de
páginas/.

Cuando se trabaja bajo una arquitectura que maneja paginación, las
direcciones que maneja el *cpu* ya no son presentadas de forma absoluta.
Los bits de cada dirección se separan en un /identificador de página/ y un
/desplazamiento/, de forma similar a lo presentado al hablar de
resolución de instrucciones en tiempo de ejecución. La principal
diferencia con lo entonces abordado es que cada proceso tendrá ya no
un único espacio en memoria, sino una multitud de páginas.

El tamaño de los marcos (y, por tanto, las páginas) debe ser una
/potencia de dos/, de modo que la *mmu* pueda discernir fácilmente la
porción de una dirección de memoria que se refiere a la /página/ del
/desplazamiento/. El rango varía, según el hardware, entre los 512
bytes ($2^9$) y 16 *mb* ($2^{24}$); al ser una potencia de dos, la *mmu*
puede separar la dirección en memoria entre los primeros $m$ bits
(referentes a la página) y los últimos $n$ bits (referentes al
desplazamiento).

#+attr_latex: width=0.7\textwidth
#+label: MEM_direccion_a_pag_y_despl
#+caption: Página y desplazamiento, en un esquema de direccionamiento de 16 bits y páginas de 512 bytes.
[[./img/ditaa/direccion_a_pag_y_despl.png]]

#+attr_latex: width=\textwidth
#+label: MEM_hardware_de_paginacion
#+caption: Esquema del proceso de paginación, ilustrando el papel de la *mmu*.
[[./img/dot/hardware_de_paginacion.png]]

Para poder realizar este mapeo, la *mmu* requiere de una  estructura
de datos denominada /tabla de páginas/ (/page table/), que /resuelve/ 
la relación entre páginas y marcos, convirtiendo una /dirección lógica/ 
(en el espacio del proceso) en la /dirección física/ (la ubicación 
en que /realmente/ se encuentra en la memoria del sistema).

Se puede tomar como ejemplo para explicar este mecanismo el esquema
presentado en la figura \ref{MEM_ejemplo_de_paginacion}
\parencite[292]{Silberschatz2010}. Éste muestra un esquema minúsculo
de paginación: un /espacio de direccionamiento/ de 32 bytes (cinco
bits), organizado en ocho páginas de cuatro bytes cada una (esto es,
la página es representada con los tres bits /más significativos/ de la
dirección, y el desplazamiento con los dos bits /menos
significativos/).

#+attr_latex: width=0.75\textwidth
#+label: MEM_ejemplo_de_paginacion
#+caption: Ejemplo (minúsculo) de paginación, con un espacio de direccionamiento de 32 bytes y páginas de cuatro bytes.
[[./img/ditaa/ejemplo_de_paginacion.png]]

El proceso que se presenta tiene una visión de la memoria como la
columna del lado izquierdo: para el proceso hay cuatro páginas, y
tiene sus datos distribuidos en orden desde la dirección $00~000$ (0)
hasta la $01~111$ (15), aunque en realidad en el sistema éstas se
encuentren desordenadas y ubicadas en posiciones no contiguas.

Cuando el proceso quiere referirse a la letra /f/, lo hace indicando
la dirección $00~101$ (5). De esta dirección, los tres bits más
significativos (001, 1 —y para la computadora, lo
/natural/ es comenzar a contar por el 0) se refieren a la página
uno, y los dos bits menos significativos (01, 1) indican al
/desplazamiento/ dentro de ésta.

La *mmu* verifica en la tabla de páginas, y encuentra que la página 1
corresponde al marco número 6 (110), por lo que traduce la dirección
lógica $00~101$ (5) a la física $11~001$ (26).

Se puede tomar la paginación como una suerte de resolución o traducción de
direcciones en tiempo de ejecución, pero con una /base/ distinta para
cada una de las páginas.

** Tamaño de la página

Ahora, si bien la fragmentación externa se resuelve al emplear
paginación, el problema de la fragmentación interna persiste: al
dividir la memoria en bloques de longitud preestablecida de $2^n$
bytes, un proceso en promedio desperdiciará $\frac{2^n}{2}$ (y, en el
peor de los casos, hasta $2^n-1$). Multiplicando esto por el número
de procesos que están en ejecución en todo momento en el sistema, para
evitar que una proporción sensible de la memoria se pierda en
fragmentación interna, se podría tomar como estrategia  emplear un tamaño
de página tan pequeño como fuera posible.

Sin embargo, la sobrecarga administrativa (el tamaño de la tabla de paginación) 
en que se incurre por
gestionar demasiadas páginas pequeñas se vuelve una limitante en
sentido opuesto:

- Las transferencias entre unidades de disco y memoria son mucho más
  eficientes si pueden mantenerse como recorridos continuos. El
  controlador de disco puede responder a solicitudes de acceso directo
  a memoria (*dma*) siempre que tanto los fragmentos en disco como en
  memoria sean continuos; fragmentar la memoria demasiado jugaría en
  contra de la eficiencia de estas solicitudes.

- El bloque de control de proceso (*pcb*) incluye la información de
  memoria. Entre más páginas tenga un proceso (aunque éstas fueran muy
  pequeñas), más grande es su *pcb*, y más información requerirá
  intercambiar en un cambio de contexto.

Estas consideraciones opuestas apuntan a que se debe mantener el
tamaño de página más grande, y se regulan con las primeras expuestas
en esta sección.

Hoy en día, el tamaño habitual de las páginas es de 4 u 8 *kb*
($2^{12}$ o $2^{13}$ bytes). Hay algunos sistemas operativos que
soportan múltiples tamaños de página — por ejemplo, Solaris puede
emplear páginas de 8 *kb* y 4 *mb* ($2^{13}$ o $2^{22}$ bytes), dependiendo
del tipo de información que se declare que almacenarán.

** Almacenamiento de la tabla de páginas

Algunos de los primeros equipos en manejar memoria paginada empleaban
un conjunto especial de registros para representar la tabla de
páginas. Esto era posible dado que eran sistemas de 16 bits, con
páginas de 8 *kb* ($2^{13}$). Esto significa que contaban únicamente
con ocho páginas posibles ($16-13 = 3; 2^3 = 8$), por lo
que resultaba sensato dedicar un registro a cada una.

En los sistemas actuales, mantener la tabla de páginas en registros
resultaría claramente imposible: teniendo un procesador de 32 bits, e
incluso si se definiera un tamaño de página /muy/ grande (por ejemplo,
4 *mb*), existirían $1~024$ páginas posibles;[fn:: 4 *mb* es $2^{22}$ bytes;
$\frac{2^{32}}{2^{22}} = 2^{10} = 1~024$.] con un tamaño de páginas
mucho más común (4 *kb*, $2^{12}$ bytes), la tabla de páginas llega a
ocupar 5 *mb*.[fn:: $\frac{2^{32}}{2^{12}} = 2^{20} = 1~048~576$, cada
entrada con un mínimo de 20 bits para la página y otros 20 para el
marco. ¡La tabla de páginas misma ocuparía 1~280 páginas!] Los
registros son muy rápidos, sin embargo, son en correspondencia muy
caros. El manejo de páginas más pequeñas (que es lo normal), y muy
especialmente el uso de espacios de direccionamiento de 64 bits,
harían prohibitivo este enfoque. Además, nuevamente, cada proceso
tiene una tabla de páginas distinta —se haría necesario hacer una
transferencia de información muy grande en cada cambio de contexto.

Otra estrategia para enfrentar esta situación es almacenar la propia
tabla de páginas en memoria, y apuntar al inicio de la tabla con un
juego de registros especiales: el /registro de base de la tabla de
páginas/ (*ptbr*, /page table base register/) y el /registro de
longitud de la tabla de páginas/ (*ptlr*, /page table length
register/).[fn:: ¿Por qué es necesario el segundo? Porque es
prácticamente imposible que un proceso emplee su espacio de
direccionamiento completo; al indicar el límite máximo de su tabla de
páginas por medio del *ptlr* se evita desperdiciar grandes cantidades
de memoria indicando todo el espacio no utiilzado.] De esta manera, en
el cambio de contexto sólo hay que cambiar estos dos registros, y
además se cuenta con un espacio muy amplio para guardar las tablas de
páginas que se necesiten. El problema con este mecanismo es la
velocidad: Se estaría penalizando a /cada acceso a memoria/ con uno
adicional —si para resolver una dirección lógica a su correspondiente
dirección física hace falta consultar la tabla de páginas en memoria,
el tiempo efectivo de acceso a memoria se duplica.

*** El buffer de traducción adelantada (*tlb*)
# <<MEM_tlb>>

La salida obvia a este dilema es el uso de un caché. Sin embargo, más
que un caché genérico, la *mmu* utiliza un caché especializado en el
tipo de información que maneja: el /buffer de traducción adelantada/ o
/anticipada/. (/Translation lookaside buffer/. El *tlb* es una
tabla asociativa (un /hash/) en memoria de alta velocidad, una suerte
de registros residentes dentro de la *mmu*, donde las /llaves/ son las
páginas y los /valores/ son los marcos correspondientes. De este modo,
las búsquedas se efectúan en tiempo constante.

El *tlb* típicamente tiene entre 64 y $1~024$ entradas. Cuando el
procesador efectúa un acceso a memoria, si la página solicitada está
en el *tlb*, la *mmu* tiene la dirección física de inmediato.[fn:: El
tiempo efectivo de acceso puede ser 10% superior al que tomaría sin
emplear paginación \parencite[295]{Silberschatz2010}.] En caso de no
encontrarse la página en el *tlb*, la *mmu* lanza un /fallo de página/
(/page fault/), con lo cual consulta de la memoria principal cuál es
el marco correspondiente. Esta nueva entrada es agregada al *tlb*; por
las propiedades de /localidad de referencia/ que se presentaron
anteriormente, la probabilidad de que las regiones más empleadas de la
memoria durante un área específica de ejecución del programa sean
cubiertas por relativamente pocas entradas del *tlb* son muy altas.

#+attr_latex: width=\textwidth
#+label: MEM_paginacion_con_tlb
#+caption: Esquema de paginación empleando un /buffer de traducción adelantada/ (*tlb*).
[[./img/dot/paginacion_con_tlb.png]]

Como sea, dado que el *tlb* es limitado en tamaño, es necesario
explicitar una política que indique dónde guardar las nuevas entradas
(esto es, qué entrada reemplazar) una vez que el *tlb* está lleno y se
produce un fallo de página. Uno de los esquemas más comunes es emplear
la entrada /menos recientemente utilizada/ (*lru*, /Least Recently
Used/), nuevamente apelando a la localidad de referencia. Esto tiene
como consecuencia necesaria que debe haber un mecanismo que
contabilice los accesos dentro del *tlb* (lo cual agrega tanto
latencia como costo). Otro mecanismo (con obvias desventajas) es el
reemplazar una página al azar. Se explicarán con mayor detalle, más
adelante, algunos de los mecanismos más empleados para este fin,
comparando sus puntos a favor y en contra.

*** Subdividiendo la tabla de páginas

Incluso empleando un *tlb*, el espacio empleado por las páginas sigue
siendo demasiado grande. Si se considera un escenario más frecuente
que el propuesto anteriormente: empleando un procesador con espacio de
direccionamiento de 32 bits, y un tamaño de página estándar (4 *kb*,
$2^{12}$), se tendría $1~048~576$ ($2^{20}$) páginas. Si cada entrada
de la página ocupa 40 bits[fn:: 20 bits identificando a la página y
otros 20 bits al marco; omitiendo aquí la necesidad de alinear los
accesos a memoria a /bytes/ individuales, que lo aumentarían a 24.]
(esto es, cinco bytes), cada proceso requeriría de 5 *mb* (cinco bytes
por cada una de las páginas) sólamente para representar su mapeo de
memoria. Esto, especialmente en procesos pequeños, resultaría más
gravoso para el sistema que los beneficios obtenidos de la paginación.

Aprovechando que la mayor parte del espacio de direccionamiento de un
proceso está típicamente vacío (la pila de llamadas y el heap), se
puede subdividir el identificador de página en dos (o más) niveles,
por ejemplo, separando una dirección de 32 bits en una /tabla externa/
de 10, una /tabla interna/ de 10, y el /desplazamiento/ de 12 bits.

#+label: MEM_paginacion_jerarquica
#+caption: Paginación en dos niveles: una tabla externa de 10 bits, tablas intermedias de 10 bits, y marcos de 12 bits (esquema común para procesadores de 32 bits).
#+attr_latex: width=\textwidth
[[./img/dot/paginacion_jerarquica.png]]

Este esquema funciona adecuadamente para computadoras con direccionamiento
de hasta 32 bits. Sin embargo, se debe considerar que cada nivel de páginas
conlleva un acceso adicional a memoria en caso de fallo de página
—emplear paginación jerárquica con un nivel externo y uno interno
implica que un fallo de página /triplica/ (y no duplica, como sería
con un esquema de paginación directo) el tiempo de acceso a
memoria. Para obtener una tabla de páginas manejable bajo los
parámetros aquí descritos en un sistema de 64 bits, se puede
/septuplicar/ el tiempo de acceso (cinco accesos /en cascada/ para
fragmentos de 10 bits, y un tamaño de página de 14 bits, más el acceso
a la página destino).

Otra alternativa es emplear /funciones digestoras/ (/hash
functions/)[fn:: Una función digestora puede definirse como $H:
U\rightarrow M$, una función que /mapea/ o /proyecta/ al conjunto $U$
en un conjunto $M$ mucho menor; una característica muy deseable de
toda función hash es que la /distribución resultante/ en $M$ sea
homogénea y tan poco dependiente de la secuencialidad de la entrada
como sea posible.] para mapear cada una de las páginas a un /espacio
muestral/ mucho más pequeño. Cada página es mapeada a una lista de
correspondencias simples.[fn:: A una lista y no a un valor único dado
que una función digestora es necesariamente proclive a presentar
/colisiones/; el sistema debe poder resolver dichas colisiones sin
pérdida de información.]

Un esquema basado en funciones digestoras ofrece características
muy deseables: el tamaño de la tabla de páginas puede variar según
crece el uso de memoria de un proceso (aunque esto requiera recalcular
la tabla con diferentes parámetros) y el número de accesos a memoria
en espacios tan grandes como el de un procesador de 64 bits se
mantiene mucho más tratable. Sin embargo, por la alta frecuencia de
accesos a esta tabla, debe elegirse un algoritmo digestor muy ágil
para evitar que el tiempo que tome calcular la posición en la tabla
resulte significativo frente a las alternativas.

** Memoria compartida

Hay muchos escenarios en que diferentes procesos pueden beneficiarse
de compartir áreas de su memoria. Uno de ellos es como mecanismo de
comunicación entre procesos (*ipc*, /inter process communication/),
en que dos o más procesos pueden intercambiar estructuras de datos
complejas sin incurrir en el costo de copiado que implicaría copiarlas
por medio del sistema operativo.

Otro caso, mucho más frecuente, es el de /compartir código/. Si un
mismo programa es ejecutado varias veces, y dicho programa no
emplea mecanismos de /código auto-modificable/, no tiene sentido que
las páginas en que se representa cada una de dichas instancias ocupe
un marco independiente —el sistema operativo puede asignar a páginas
de diversos procesos /el mismo conjunto de marcos/, con lo cual puede
aumentar la capacidad percibida de memoria.

Y si bien es muy común compartir los /segmentos de texto/ de los
diversos programas que están en un momento dado en ejecución en la
computadora, este mecanismo es todavía más útil cuando se usan
/bibliotecas del sistema/: hay bibliotecas que son empleadas por una
gran cantidad de programas[fn:: Algunos ejemplos sobresalientes
podrían ser la =libc= o =glibc=, que proporciona las funcinoes
estándar del lenguaje C y es, por tanto, requerida por casi todos los
programas del sistema; los diferentes entornos gráficos (en los Unixes
modernos, los principales son =Qt= y =Gtk=); bibliotecas para el
manejo de cifrado (=openssl=), compresión (=zlib=), imágenes
(=libpng=, =libjpeg=), etcétera.].

#+attr_latex: width=\textwidth
#+label: MEM_memoria_compartida
#+caption: Uso de memoria compartida: tres procesos comparten la memoria ocupada por el texto del programa (azul), difieren sólo en los datos.
[[./img/ditaa/memoria_compartida.png]]

Claro está, para ofrecer este modelo, el sistema operativo debe 
garantizar que las páginas correspondientes a las /secciones de texto/
(el código del programa) sean de sólo lectura.

Un programa que está desarrollado y compilado de forma que permita que
todo su código sea de sólo lectura posibilita que diversos procesos
entren a su espacio en memoria sin tener que sincronizarse con otros
procesos que lo estén empleando.

*** Copiar al escribir (/copy on write/, /CoW/)

En los sistemas Unix, el mecanismo más frecuentemente utilizado para
crear un nuevo proceso es el empleo de la llamada al sistema
=fork()=. Cuando es invocado por un proceso, el sistema operativo crea
un nuevo proceso /idéntico/ al que lo llamó, diferenciándose
únicamente en /el valor entregado/ por la llamada a =fork()=. Si
ocurre algún error, el sistema entrega un número negativo (indicando
la causa del error). En caso de ser exitoso, el proceso nuevo (o
proceso /hijo/) recibe el valor =0=, mientras que el preexistente (o
proceso padre) recibe el *pid* (número identificador de proceso) del
hijo. Es frecuente encontrar el siguiente código:

#+begin_src c
  /* (...) */
  int pid;
  /* (...) */
  pid = fork();
  if (pid == 0) {
    /* Soy el proceso hijo */
    /* (...) */
   } else if (pid < 0) {
    /* Ocurrió un error, no se creó un proceso hijo */
   } else {
    /* Soy el proceso padre */
    /* La variable 'pid' tiene el PID del proceso hijo */
    /* (...) */
   }
#+end_src

Este método es incluso utilizado normalmente para crear nuevos
procesos, transfiriendo el /ambiente/ (variables, por ejemplo, que
incluyen cuál es la /entrada/ y /salida/ estándar de un proceso, esto
es, a qué terminal están conectados, indispensable en un sistema
multiusuario). Frecuentemente, la siguiente instrucción que ejecuta un
proceso hijo es =execve()=, que carga a un nuevo programa sobre el
actual y transfiere la ejecución a su primera instrucción.

Cuesta trabajo comprender el por qué de esta lógica si no es por el
empleo de la memoria compartida: el costo de =fork()= en un sistema
Unix es muy bajo, se limita a crear las estructuras necesarias en la
memoria del núcleo. Tanto el proceso padre como el proceso hijo
comparten /todas/ sus páginas de memoria, como lo ilustra la figura
\ref{MEM_cow_recien_hecho}, sin embargo, siendo dos
procesos independientes, no deben poder modificarse más que por los
canales explícitos de comunicación entre procesos.

Esto ocurre así gracias al mecanismo llamado /copiar al escribir/
(frecuentemente referido por sus siglas en inglés, /CoW/). Las páginas
de memoria de ambos procesos son las mismas /mientras sean sólo
leídas/. Sin embargo, si uno de los procesos modifica cualquier dato
en una de estas páginas, ésta se copia a un nuevo marco, y deja de ser
una página compartida, como se puede ver en la figura
\ref{MEM_cow_pagina_modificada}. El resto de las páginas seguirá
siendo compartido. Esto se puede lograr marcando /todas/ las páginas
compartidas como /sólo lectura/, con lo cual cuando uno de los dos
procesos intente modificar la información de alguna página se generará
un fallo. El sistema operativo, al notar que esto ocurre sobre un
espacio CoW, en vez de responder al fallo terminando al proceso,
copiará sólo la página en la cual se encuentra la dirección de memoria
que causó el fallo, y esta vez marcará la página como /lectura y
escritura/.

#+begin_latex
\begin{figure}
\centering
\subfigure[Inmediatamente después de la creación del proceso hijo por \texttt{fork()} \label{MEM_cow_recien_hecho}]{\includegraphics[width=0.9\textwidth]{img/ditaa/cow_recien_hecho.png}}
\subfigure[Cuando el proceso hijo modifica información en la primer página de su memoria, se crea como una página nueva. \label{MEM_cow_pagina_modificada}]{\includegraphics[width=0.9\textwidth]{img/ditaa/cow_pagina_modificada.png}}
\caption {Memoria de dos procesos en un sistema que implementa \emph{copiar al escribir}.}
\label{MEM_modelos_de_hilos}
\end{figure}
#+end_latex

#+begin_html
<table class="figure" style="margin-right:auto; margin-left:auto;" >
  <tr>
    <td style="padding: 1em">
      <span id="MEM_cow_recien_hecho">
        <p><img src="./img/ditaa/cow_recien_hecho.png" alt="./img/ditaa/cow_recien_hecho.png" /></p>
        <p>Inmediatamente después de la creación del proceso hijo por <tt>fork()</tt></p>
      </span>
    </td>
  </tr><tr>
    <td style="padding: 1em">
      <span id="MEM_cow_pagina_modificada">
        <p><img src="./img/ditaa/cow_pagina_modificada.png" alt="./img/ditaa/cow_pagina_modificada.png" /></p>
        <p>Cuando el proceso hijo modifica información en la primer página de su memoria, se crea como una página nueva.</p>
      </span>
    </td>
  </tr>
  <caption style="caption-side: bottom">Tres modelos de mapeo de hilos a procesos. (Imágenes: Beth Plale; ver <em>Otros recursos</em>)</caption>
</table>
#+end_html

Incluso cuando se ejecutan nuevos programas mediante =execve()=,
es posible que una buena parte de la memoria se mantenga compartida,
por ejemplo, al referirse a copias de bibliotecas de sistema.

* Memoria virtual
# <<MEM_virtual>>

Varios de los aspectos mencionados en la sección \ref{MEM_paginacion}
(/paginación/) van conformando a lo que se conoce como /memoria
virtual/: en un sistema que emplea paginación, un proceso
no conoce su dirección en memoria relativa a otros procesos, sino que
trabajan con una /idealización/ de la memoria, en la cual ocupan el
espacio completo de direccionamiento, desde el cero hasta el límite
lógico de la arquitectura, independientemente del tamaño físico de la
memoria disponible.

Y si bien en el modelo mencionado de paginación los diferentes
procesos pueden /compartir/ regiones de memoria y /direccionar/ más
memoria de la físicamente disponible, no se ha presentado aún la
estrategia que se emplearía cuando el total de páginas solicitadas por
todos los procesos activos en el sistema superara el total de espacio
físico. Es ahí donde entra en juego la /memoria virtual/: para ofrecer
a los procesos mayor espacio en memoria de con el que se cuenta
físicamente, el sistema emplea espacio en /almacenamiento secundario/
(típicamente, disco duro), mediante un esquema de /intercambio/
(/swap/) guardando y trayendo páginas enteras.

#+label: MEM_esquema_gral_mem_virtual
#+caption: Esquema general de la memoria, incorporando espacio en almacenamiento secundario, representando la memoria virtual.
#+attr_latex: width=0.6\textwidth
[[./img/dot/esquema_gral_mem_virtual.png]]

Es importante apuntar que la memoria virtual es gestionada /de forma
automática y transparente/ por el sistema operativo. No se hablaría
de memoria virtual, por ejemplo, si un proceso pide explícitamente
intercambiar determinadas páginas.

Puesto de otra manera: del mismo modo que la segmentación (sección
\ref{MEM_segmentacion}) permitió hacer mucho más cómodo y útil al
intercambio (\ref{MEM_swap}) por medio del intercambio parcial
(\ref{MEM_intercambio_parcial}), permitiendo que continuara la
ejecución del proceso, incluso con ciertos segmentos /intercambiados/
(/swappeados/) a disco, la memoria virtual lo hace aún más conveniente
al aumentar la /granularidad/ del intercambio: ahora ya no se
enviarán a disco secciones lógicas completas del proceso (segmentos),
sino que se podrá reemplazar página por página, aumentando significativamente
el rendimiento resultante. Al emplear la memoria virtual, de
cierto modo la memoria física se vuelve sólo una /proyección parcial/
de la memoria lógica, potencialmente mucho mayor a ésta.

Técnicamente, cuando se habla de memoria virtual, no se está
haciendo referencia a un /intercambiador/ (/swapper)/, sino al /paginador/.

** Paginación sobre demanda
# <<MEM_pag_sobre_demanda>>

La memoria virtual entra en juego desde la carga misma del proceso.
Se debe considerar que hay una gran cantidad de /código durmiente/
o /inalcanzable/: aquel
que sólo se emplea eventualmente, como el que responde ante una
situación de excepción o el que se emplea sólo ante circunstancias
particulares (por ejemplo, la exportación de un documento a
determinados formatos, o la verificación de que no haya tareas
pendientes antes de cerrar un programa). Y si bien a una computadora
le sería imposible ejecutar código que no esté cargado en
memoria,[fn:: Una computadora basada en la arquitectura von Neumann,
como prácticamente todas las existen hoy en día, no puede /ver/
directamente más que la memoria principal.] éste sí puede
comenzar a ejecutarse sin estar /completamente/ en memoria: basta con
haber cargado la página donde están las instrucciones que permiten
continuar con su ejecución actual.

La /paginación sobre demanda/ significa que, para comenzar a ejecutar
un proceso, el sistema operativo carga /solamente la porción
necesaria/ para comenzar la ejecución (posiblemente una página o
ninguna), y que a lo largo de la ejecución, el paginador /es
flojo/:[fn:: En cómputo, muchos procesos pueden determinarse como
/ansiosos/ (/eager/), cuando buscan realizar todo el trabajo que
puedan desde el inicio, o /flojos/ (/lazy/), si buscan hacer el
trabajo mínimo en un principio y diferir para más tarde tanto como sea
posible.] sólo carga a memoria las páginas cuando van a ser
utilizadas. Al emplear un paginador /flojo/, las páginas que no sean
requeridas nunca serán siquiera cargadas a memoria.

La estructura empleada por la *mmu* para implementar un paginador flojo
es muy parecida a la descrita al hablar del buffer de tradución
adelantada (sección \ref{MEM_tlb}): la /tabla de páginas/ incluirá un
/bit de validez/, indicando para cada página del proceso si está
presente o no en memoria. Si el proceso intenta emplear una página que
esté marcada como no válida, esto causa un fallo de página, que lleva
a que el sistema operativo lo suspenda y traiga a memoria la página
solicitada para luego continuar con su ejecución:

#+label: MEM_respuesta_a_fallo_de_pagina
#+caption: Pasos que atraviesa la respuesta a un fallo de página.
#+attr_latex: width=0.6\textwidth
[[./img/dot/respuesta_a_fallo_de_pagina.png]]

1. Verifica en el *pcb* si esta solicitud corresponde a una página que
   ya ha sido asignada a este proceso.

2. En caso de que la referencia sea inválida, se termina el proceso.

3. Procede a traer la página del disco a la memoria. El primer paso es
   buscar un marco disponible (por ejemplo, por medio de una tabla de
   asignación de marcos).

4. Solicita al disco la lectura de la página en cuestión hacia el
   marco especificado.

5. Una vez que finaliza la lectura de disco, modifica tanto al *pcb*
   como al *tlb* para indicar que la tabla está en memoria.

6. Termina la suspensión del proceso, continuando con la instrucción
   que desencadenó al fallo. El proceso puede continuar sin notar 
   que la página había sido intercambiada.

Llevando este proceso al extremo, se puede pensar en un sistema de
/paginación puramente sobre demanda/ (/pure demand paging/): en un
sistema así, /ninguna/ página llegará al espacio de un proceso si no
es mediante de un fallo de página. Un proceso, al iniciarse, comienza
su ejecución /sin ninguna página en memoria/, y con el apuntador de
siguiente instrucción del procesador apuntando a una dirección que no
está en memoria (la dirección de la rutina de /inicio/). El sistema operativo 
responde cargando esta primer página, y conforme avanza el flujo del 
programa, el proceso irá ocupando el espacio real que empleará.

** Rendimiento

La paginación sobre demanda puede impactar fuertemente el rendimiento
de un proceso -se ha explicado ya que un acceso a disco es varios miles de
veces más lento que el acceso a memoria. Es posible calcular el tiempo de
acceso efectivo a memoria ($t_e$) a partir de la probabilidad que en
un proceso se presente un fallo de página ($0 \le p \le 1$),
conociendo el tiempo de acceso a memoria ($t_a$) y el tiempo que toma
atender a un fallo de página ($t_f$):

$$t_e = (1-p)t_a + pt_f$$

Ahora bien, dado que $t_a$ ronda hoy en día entre los 10 y 200ns,
mientras que $t_f$ está más bien cerca de los 8 ms (la latencia típica
de un disco duro es de 3 ms, el tiempo de posicionamiento de cabeza de
5ms, y el tiempo de transferencia es de 0.05 ms), para propósitos
prácticos se puede ignorar a $t_a$. Con los valores presentados,
seleccionando el mayor de los $t_a$ presentados, si sólo un acceso a
memoria de cada 1000 ocasiona un fallo de página (esto es,
$p=\frac{1}{1~000}$):

$$t_e = (1-\frac{1}{1~000}) \times 200ns + \frac{1}{1~000} \times 8~000~000ns$$

$$t_e = 199.8ns + 8~000ns = 8~199.8ns$$

Esto es, en promedio, se tiene un tiempo efectivo de acceso a memoria
/40 veces/ mayor a que si no se empleara este mecanismo. Con estos
mismos números, para mantener la degradación de rendimiento por acceso
a memoria por debajo de 10%, se debería reducir la probabilidad de
fallos de página a $\frac{1}{399~990}$.

Cabe mencionar que esta repercusión al rendimiento no necesariamente
significa que una proporción relativamente alta de fallos de página
para un proceso afecte negativamente a todo el sistema —el mecanismo
de paginación sobre demanda permite, al no requerir que se tengan en
memoria todas las páginas de un proceso, que haya /más procesos
activos/ en el mismo espacio en memoria, aumentando el grado de
multiprogramación del equipo. De este modo, si un proceso se ve obligado a esperar por 8
ms a que se resuelva un fallo de página, durante ese tiempo pueden seguirse
ejecutando los demás procesos.

*** Acomodo de las páginas en disco
# <<MEM_acomodo_de_paginas>>
El cálculo recién presentado, además, asume que el acomodo de las
páginas en disco es óptimo. Sin embargo, si para llegar a una página
hay que resolver la dirección que ocupa en un sistema de archivos
(posiblemente navegar una estructura de directorio), y si el espacio
asignado a la memoria virtual es compartido con los archivos en disco,
el rendimiento sufrirá adicionalmente.

Una de las principales deficiencias estructurales en este sentido de
los sistemas de la familia Windows es que el espacio de almacenamiento
se asigna en el espacio libre del sistema de archivos. Esto lleva a
que, conforme crece la fragmentación del disco, la memoria virtual
quede esparcida por todo el disco duro. La generalidad de sistemas tipo 
Unix, en contraposición, reservan una partición de disco /exclusivamente/ 
para paginación.

** Reemplazo de páginas

Si se aprovechan las características de la memoria virtual para
aumentar el grado de multiprogramación, como se explicó en la sección
anterior, se presenta un problema: al /sobre-comprometer/ memoria,
en determinado momento, los procesos que están en ejecución pueden
caer en un patrón que requiera cargarse a memoria física páginas por
un mayor uso de memoria que el que hay físicamente disponible.

Y si se tiene en cuenta que uno de los objetivos del sistema operativo
es otorgar a los usuarios la /ilusión/ de una computadora dedicada a
sus procesos, no sería aceptable terminar la ejecución de un proceso
ya aceptado y cuyos requisitos han sido aprobados, porque no hay
suficiente memoria. Se vuelve necesario encontrar una forma justa y
adecuada de llevar a cabo un /reemplazo de páginas/ que permita
continuar satisfaciendo sus necesidades.

El reemplazo de páginas es una parte fundamental de la paginación, 
ya que es la pieza que posibilita una verdadera separación
entre memoria lógica y física. El mecanismo básico a ejecutar es simple: 
si todos los marcos están ocupados, el sistema deberá encontrar una página 
que pueda liberar (una /página víctima/) y llevarla al espacio de 
intercambio en el disco. Luego, se puede emplear el espacio recién
liberado para /traer de vuelta/ la página requerida, y continuar con la
ejecución del proceso.

Esto implica una /doble/ transferencia al disco (una para grabar
la página víctima y una para traer la página de reemplazo), y por
tanto, a una doble demora.

Se puede, con un mínimo de /burocracia/ adicional (aunque requiere de
apoyo de la *mmu*): implementar un mecanismo que disminuya la probabilidad
de tener que realizar esta doble transferencia: agregar un /bit
de modificación/ o /bit de página sucia/ (/dirty bit/) a la tabla de
páginas. Este bit se marca como apagado siempre que se carga una
página a memoria, y es automáticamente encendido por hardware cuando se realiza un
acceso de escritura a dicha página.

Cuando el sistema operativo elige una página víctima, si su /bit de
página sucia/ está encendido, es necesario grabarla al disco, pero si
está apagado, se garantiza que la información en disco es idéntica  a su copia en 
memoria, y permite ahorrar la mitad del tiempo de transferencia.

Ahora bien, ¿cómo decidir qué páginas reemplazar marcándolas como
/víctimas/ cuando hace falta? Para esto se debe implementar un
/algoritmo de reemplazo de páginas/. La característica que se busca en
este algoritmo es que, para una patrón de accesos dado, permita obtener el
menor número de fallos de página.

De la misma forma como se realizó la descripción de los algoritmos de
planificación de procesos, para analizar los algoritmos de reemplazo
se usará una /cadena de referencia/, esto es, una lista de referencias
a memoria. Estas cadenas modelan el comportamiento de un conjunto de
procesos en el sistema, y, obviamente, diferentes comportamientos
llevarán a resultados distintos.

Hacer un volcado y trazado de ejecución en un sistema real puede dar
una enorme cantidad de información, del orden de un millón de accesos
por segundo. Para reducir esta información en un número más tratable,
se puede simplificar basado en que no interesa cada referencia a una /dirección/ 
de memoria,  sino cada referencia a una /página/ diferente.

Además, varios accesos a direcciones de memoria en la misma página no
causan efecto en el estado. Se puede tomar como un sólo acceso a todos
aquellos que ocurren de forma consecutiva (esto es, sin llamar a
ninguna otra página, no es necesario que sean en instrucciones
consecutivas) a una misma página.

Para analizar a un algoritmo de reemplazo, si se busca la
cantidad de fallos de página producidos, además de la cadena de
referencia, es necesario conocer la cantidad de páginas y marcos
del sistema que se está modelando. Por ejemplo, considérese la cadena
de 12 solicitudes:

#+begin_quote
1, 4, 3, 4, 1, 2, 4, 2, 1, 3, 1, 4
#+end_quote

Al recorrerla en un sistema con cuatro o más marcos,
sólo se presentarían cuatro fallos (el fallo inicial que hace
que se cargue por primera vez cada una de las páginas). Si, en el otro
extremo, se cuenta con sólo un marco, se presentarían 12 fallos,
dado que a cada solicitud se debería reemplazar el único marco
disponible. El rendimiento evaluado sería en los casos de que se cuenta
con dos o tres marcos.

*** Anomalía de Belady

Un fenómeno interesante que se presenta con algunos algoritmos es la
/anomalía de Belady/, publicada en 1969: si bien la lógica indica que
a mayor número de marcos disponibles se tendrá una menor cantidad de
fallos de página, como lo ilustra la figura
\ref{MEM_expectativa_fallas_marcos}, con algunas de cadenas de
referencia y bajo ciertos algoritmos puede haber una /regresión/ o degradación, en
la cual la cantidad de fallos aumenta aún con una mayor cantidad de
marcos, como se puede ver en la figura \ref{MEM_anomalia_belady}.

Es importante recalcar que si bien la anomalía de Belady se presenta
como un problema importante ante la evaluación de los algoritmos, en
\parencite[559-569]{LaRed2001} se puede observar que en
simulaciones con características más cercanas a las de los patrones
reales de los programas, su efecto observado es prácticamente nulo.

Para los algoritmos que se presentan a continuación, se asumirá una memoria
con tres marcos, y con la siguiente cadena de referencia:

#+begin_quote
7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1
#+end_quote

#+begin_latex
\begin{figure}
\centering
\subfigure[Relación ideal entre el número de marcos y fallos de página. \label{MEM_expectativa_fallas_marcos}]{\includegraphics[width=0.7\textwidth]{img/gnuplot/expectativa_fallas_contra_marcos.png}}
\subfigure[Comportamiento del algoritmo *fifo* exhibiendo la anomalía de Belady al pasar de tres a cuatro marcos. \label{MEM_anomalia_belady}]{\includegraphics[width=0.7\textwidth]{img/gnuplot/anomalia_belady.png}}
\caption {Anomalía de Belady, empleando la cadena de referencia \emph{1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5} (Belady, 1969).}
\label{MEM_expectativas_y_belady}
\end{figure}
#+end_latex

#+begin_html
<table class="figure" style="margin-right:auto; margin-left:auto;" >
  <tr>
    <td style="padding: 1em">
      <span id="MEM_expectativa_fallas_marcos">
        <p><img src="./img/gnuplot/expectativa_fallas_marcos.png" alt="./img/gnuplot/expectativa_fallas_marcos.png" /></p>
        <p>Relación ideal entre el número de marcos y fallos de página.</p>
      </span>
    </td>
  </tr><tr>
    <td style="padding: 1em">
      <span id="MEM_anomalia_belady">
        <p><img src="./img/gnuplot/anomalia_belady.png" alt="./img/gnuplot/anomalia_belady.png" /></p>
        <p>Comportamiento del algoritmo <b>fifo</b> exhibiendo la anomalía de Belady al pasar de tres a cuatro marcos.</p>
      </span>
    </td>
  </tr>
  <caption style="caption-side: bottom">Anomalía de Belady, empleando la cadena de referencia <em>1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5</em> (Belady, 1969).</caption>
</table>
#+end_html

*** Primero en entrar, primero en salir (*fifo*)
# <<MEM_FIFO>>

El algoritmo de más simple y de obvia implementación es, nuevamente,
el *fifo*: al cargar una página en memoria, se toma nota de en qué
momento fue cargada, y cuando sea necesario reemplazar una página,
se elige la que haya sido cargada hace más tiempo.

Partiendo de un estado inicial en que las tres páginas están
vacías, necesariamente las tres primeras referencias a distintas
páginas de memoria (7, 0, 1) causarán fallos de página. La siguiente
(2) causará uno, pero la quinta referencia (0) puede ser satisfecha
sin requerir una nueva transferencia.

#+attr_latex: width=\textwidth
#+label: MEM_reemplazo_pag_fifo
#+caption: Algoritmo *fifo* de reemplazo de páginas.
[[./img/ditaa/reemplazo_pag_fifo.png]]

La principal ventaja de este algoritmo es, como ya se ha
mencionado, la simplicidad, tanto para programarlo como para
comprenderlo. Su implementación puede ser tan simple como una lista
ligada circular, cada elemento que va recibiendo se agrega en el
último elemento de la lista, y se "empuja" el apuntador para
convertirlo en la cabeza. Su desventaja, claro está, es que no toma
en cuenta la historia de las últimas solicitudes, por lo que puede
causar un bajo rendimiento. Todas las páginas tienen la misma
probabilidad de ser reemplazadas, sin importar su frecuencia de uso.

Con las condiciones aquí presentadas, un esquema *fifo* causará 15
fallos de página en un total de 20 accesos requeridos.

El algoritmo *fifo* es vulnerable a la anomalía de Belady. La figura
\ref{MEM_anomalia_belady} ilustra este fenómeno al pasar de tres a
cuatro marcos.

La prevalencia de cadenas que desencadenan la anomalía de Belady fue
uno de los factores principales que llevaron al diseño de nuevos
algoritmos de reemplazo de páginas.

*** Reemplazo de páginas óptimo (*opt*, *min*)

Un segundo algoritmo, de interés puramente teórico, fue
propuesto, y es típicamente conocido como *opt* o *min*. Bajo este
algoritmo, el enunciado será elegir como página víctima a aquella
página que /no vaya a ser utilizada/ por un tiempo máximo (o nunca más).

#+attr_latex: width=\textwidth
#+label: MEM_reemplazo_pag_opt
#+caption: Algoritmo óptimo de reemplazo de páginas (*opt*).
[[./img/ditaa/reemplazo_pag_opt.png]]

Si bien este algoritmo está demostrado como óptimo o mínimo, se
mantiene como curiosidad teórica porque requiere conocimiento /a priori/
de las necesidades a futuro del sistema —y si esto es impracticable ya en los
algoritmos de despachadores, lo será mucho más con un
recurso de reemplazo tan dinámico como la memoria.

Su principal utilidad reside en que ofrece una cota mínima:
calculando el número de fallos que se presentan al seguir *opt*,
es posible ver qué tan cercano resulta otro algoritmo respecto al caso
óptimo. Para esta cadena de referencia, y con tres páginas, se tiene
un total de nueve fallos.

*** Menos recientemente utilizado (*lru*)
Este esquema se ha revisado en diversos mecanismos relacionados con la
administración de memoria. Busca acercarse a *opt* /prediciendo/ cuándo
será la próxima vez en que se emplee cada una de las páginas que
tiene en memoria basado en la /historia reciente/ de su
ejecución.

Cuando necesita elegir una página víctima, *lru* elige la página que no
ha sido empleada hace más tiempo.

#+attr_latex: width=\textwidth
#+label: MEM_reemplazo_pag_lru
#+caption: Algoritmo reemplazo de páginas menos recientemente utilizadas (*lru*).
[[./img/ditaa/reemplazo_pag_lru.png]]

Para la cadena de referencia, *lru* genera 11 fallos, en el punto
medio entre *opt* y *fifo*.

Una observación interesante puede ser que para una cadena $S$ y su
/cadena espejo/ (invertida) $R^S$, el resultado de evaluar $S$
empleando *lru* es igual al de evaluar $R^S$ con *opt*, y viceversa.

La principal debilidad de *lru* es que para su implementación requiere
apoyo en hardware[fn:: Dada la frecuencia con que se efectúan
referencias a memoria, emplear un mecanismo puramente en software para
actualizar las entradas de los marcos resultaría inaceptablemente
lento.] sensiblemente más complejo que *fifo*. Una implementación
podría ser agregar un contador a cada uno de los marcos, actualizarlo
siempre al hacer una referenciar a dicha página, y elegir como víctima
a la página con un menor conteo. Este mecanismo tiene la desventaja de
que, en presencia de una gran cantidad de páginas, tiene que
recorrerlas todas para buscar la más /envejecida/.

Otro mecanismo es emplear una lista doblemente ligada con dos métodos
de acceso: lista y /stack/. Cada vez que se haga referencia a una
página, ésta se mueve a la cabeza del /stack/, y cada vez que se
busque una página víctima, se selecciona a aquella que esté en el
extremo /inferior/ del /stack/ (tomándolo como lista). Este mecanismo
hace un poco más cara la actualización (pueden requerirse hasta seis
modificaciones), pero encuentra la página víctima en tiempo constante.

Se ha demostrado que *lru* y *opt* están libres de la anomalía de Belady,
dado que, para $n$ marcos, las páginas que estarían en memoria son un
subconjunto estricto de las que estarían con $n+1$ marcos.

*** Más frecuentemente utilizada (*mfu*)/Menos frecuentemente utilizada (*lfu*)

Estos dos algoritmos se basan en mantener un contador, tal como lo
hace *lru*, pero en vez de medir el tiempo, miden la /cantidad/ de
referencias que se han hecho a cada página.

El *mfu* parte de la lógica que, si una página fue empleada muchas
veces, probablemente vuelva a ser empleada muchas veces más; *lfu*
parte de que una página que ha sido empleada pocas veces es
probablemente una página recién cargada, y va a ser empleada en el
futuro cercano.

Estos dos algoritmos son tan caros de implementar como *lru*, y su
rendimiento respecto a *opt* no es tan cercana, por lo cual casi no son
empleados.

*** Aproximaciones a *lru*

Dada la complejidad que presenta la implementación de *lru* en
hardware, los siguientes sistemas buscan una /aproximación/ a éste.

- Bit de referencia :: Esta es una aproximación bastante
     común. Consiste en que todas las entradas de la tabla de páginas
     tengan un bit adicional, al que llamaremos /de referencia/ o /de acceso/.
     Al iniciar la ejecución, todos los bits de referencia están apagados
     (0). Cada vez que se referencia a un marco, su bit de referencia
     se enciende (esto, en general, lo realiza el hardware).

     El sistema operativo invoca periódicamente a que se apaguen
     nuevamente todos los bits de referencia. En caso de presentarse
     un fallo de página, se elige por *fifo* sobre el subconjunto de
     marcos que no hayan sido referenciados en el periodo actual (esto
     es, entre todos aquellos para los cuales el bit de referencia sea
     0).

- Columna de referencia :: Una mejoría casi trivial sobre la anterior
     consiste en agregar /varios/ bits de referencia, conformándose
     como una /columna/: en vez de descartar su valor cada vez que
     transcurre el periodo determinado, el valor de la columna de
     referencia es desplazado a la derecha, descartando el bit más
     bajo (una actualización sólo modifica el bit más significativo).
     Por ejemplo, con una implementación de cuatro bits, un marco que
     no ha sido empleado en los últimos cuatro periodos tendría el
     valor $0~000$, mientras que un marco que sí ha sido referenciado
     los últimos cuatro periodos tendría $1~111$. Un marco que fue
     empleado hace cuatro y tres periodos, pero a partir entonces ya
     no, tendría el 0011.

     Cuando el sistema tenga que elegir a una nueva página víctima,
     lo hará de entre el conjunto que tenga un número más bajo.

     La parte de mantenimiento de este algoritmo es muy simple;
     recorrer una serie de bits es una operación muy
     sencilla. Seleccionar el número más bajo requiere una pequeña
     búsqueda, pero sigue resultando mucho más sencillo que *lru*.

- Segunda oportunidad (o /reloj/) :: El algoritmo de la segunda
     oportunidad trabaja también basado en un bit de referencia y un
     recorrido tipo *fifo*. La diferencia en este caso es que, al
     igual que hay eventos que /encienden/ a este bit (efectuar una
     referencia al marco), hay otros que lo /apagan/:

     se mantiene un apuntador a la /próxima víctima/, y cuando el
     sistema requiera efectuar un reemplazo, éste verificará si el
     marco al que apunta tiene el bit de referencia encendido o
     apagado. En caso de estar apagado, el marco es seleccionado como
     víctima, pero en caso de estar encendido (indicando que fue
     utilizado recientemente), se le da una /segunda oportunidad/: el
     bit de referencia se apaga, el apuntador de víctima potencial
     avanza una posición, y vuelve a intentarlo.

     A este algoritmo se le llama también /de reloj/ porque puede
     implementarse como una lista ligada circular, y el apuntador
     puede ser visto como una manecilla. La manecilla avanza sobre la
     lista de marcos buscando uno con el bit de referencia apagado, y
     apagando a todos a su paso.

     En el peor caso, el algoritmo de /segunda oportunidad/ degenera
     en *fifo*.

- Segunda oportunidad mejorada :: El bit de referencia puede
     ampliarse con un /bit de modificación/, dándonos las siguientes
     combinaciones, en orden de preferencia:

     - (0, 0) :: No ha sido utilizado ni modificado
                 recientemente. Candidato ideal para su reemplazo.

     - (0,1) :: No ha sido utilizada recientemente, pero está
                modificada. No es tan buena opción, porque es
                necesario escribir la página a disco antes de
                reemplazarla, pero puede ser elegida.

     - (1,0) :: El marco está /limpio/, pero fue empleado
                recientemente, por lo que probablemente se vuelva a
                requerir pronto.

     - (1,1) :: Empleada recientemente y /sucia/ —sería necesario
                escribir la página a disco antes de reemplazar, y
                probablemente vuelva a ser requerida pronto. Hay que
                evitar reemplazarla.

     La lógica para encontrar una página víctima es similar a la
     /segunda oportunidad/, pero busca reducir el costo de E/S. Esto
     puede requerir, sin embargo, dar hasta cuatro vueltas (por cada
     una de las listas) para elegir la página víctima.

*** Algoritmos con manejo de buffers

Un mecanismo que se emplea cada vez con mayor frecuencia es que el
sistema no espere a enfrentarse a la necesidad de reemplazar un
marco, sino que proactivamente busque tener siempre espacio vacío en
memoria. Para hacerlo, conforme la carga lo permite, el sistema
operativo busca las páginas /sucias/ más proclives a ser llevadas a
disco y va actualizando el disco (y marcándolas nuevamente como
/limpias/). De este modo, cuando tenga que traer de vuelta una página,
siempre habrá espacio donde ubicarla sin tener que esperar a que se
transfiera una para liberarla.

** Asignación de marcos

Abordando el problema prácticamente por el lado opuesto al del
reemplazo de páginas, ¿cómo se asignan los marcos existentes a los
procesos del sistema? Esto es, ¿qué esquemas se pueden definir para
que la asignación inicial (y, de ser posible, en el transcurso de la
ejecución) sea adecuada?

Por ejemplo, usando esquema sencillo: un sistema con
$1~024$ *kb* de memoria, compuesta de 256 páginas de 4096 bytes cada una, y
basado en paginación puramente sobre demanda.

Si el sistema operativo ocupa 248 *kb*, el primer paso será reservar las
62 páginas que éste requiere, y destinar las 194 páginas restantes
para los procesos a ejecutar.

Conforme se van lanzando y comienzan a ejecutar los procesos, cada
vez que uno de ellos genere un fallo de página, se le irá asignando
uno de los marcos disponibles hasta causar que la memoria entera esté
ocupada. Claro está, cuando un proceso termine su ejecución, todos
los marcos que tenía asignados volverán a la lista de marcos libres.

Una vez que la memoria esté completamente ocupada (esto es, que haya
194 páginas ocupadas por procesos), el siguiente fallo de página
invocará a un algoritmo de reemplazo de página, que elegirá una de las
194.[fn:: En realidad, dentro de la memoria del sistema operativo, al
igual que la de cualquier otro proceso, hay regiones que deben
mantenerse residentes y otras que pueden paginarse. Se puede,
simplificando, omitir por ahora esa complicación y asumir que el
sistema operativo completo se mantendrá siempre en memoria]

Este esquema, si bien es simple, al requerir una gran cantidad de
fallos de página explícitos puede penalizar el rendimiento del
sistema —el esquema puede resultar /demasiado flojo/, no le vendría
mal ser un poco más /ansioso/ y asignar, de inicio, un número
determinado como mínimo utilizable de marcos.

*** Mínimo de marcos

Si un proceso tiene asignados muy pocos marcos, su rendimiento
indudablemente se verá afectado. Hasta ahora se ha supuesto que cada
instrucción puede causar un sólo fallo de página, pero la realidad es
más compleja. Cada instrucción del procesador puede, dependiendo de
la arquitectura, desencadenar varias solicitudes y potencialmente
varios fallos de página.

Todas las arquitecturas proporcionan instrucciones de referencia
directa a memoria (instrucciones que permiten especificar una
dirección de memoria para leer o escribir) — esto significa que todas
requerirán que, para que un proceso funcione adecuadamente, tenga por
lo menos dos marcos asignados: en caso de que se le permitiera sólo
uno, si la instrucción ubicada en =0x00A2C8= solicita la carga de
=0x043F00=, ésta causaría dos fallos: el primero, cargar al marco la
página =0x043=, y el segundo, cargar nuevamente la página =0x00A=,
necesario para leer la siguiente instrucción a ejecutar del programa
(=0x00A2CC=, asumiendo palabras de 32 bits).

Algunas arquitecturas, además, permiten /referencias indirectas a
memoria/, esto es, la dirección de carga puede solicitar la
dirección /que está referenciada/ en =0x043F00=. El procesador
tendría que recuperar esta dirección, y podría encontrarse con que
hace referencia a una dirección en otra página (por ejemplo,
=0x010F80=). Cada nivel de indirección que se permite aumenta en uno
el número de páginas que se deben reservar como mínimo por proceso.

Algunas arquitecturas, particularmente las más antiguas,[fn:: Aquellas
diseñadas antes de que la velocidad del procesador se distanciara
tanto del tiempo de acceso a memoria.] permiten que tanto los
operandos de algunas instrucciones aritméticas como su resultado sean
direcciones de memoria (y no operan estrictamente sobre los registros,
como las arquitecturas *risc*). En éstas, el mínimo debe también tener
este factor en cuenta: si en una sola instrucción es posible sumar dos
direcciones de memoria y guardar el resultado en una adicional, el
mínimo a reservar es de cuatro marcos: uno para el flujo del programa,
otro para el primer operando, uno para el segundo operando, y el
último para el resultado.

*** Esquemas de asignación

Ahora, una vez establecido el número mínimo de marcos por proceso,
¿cómo determinar el nivel /deseable/?

Partiendo de que el rendimiento de un proceso será mejor entre menos
fallos de paginación cause, se podría intentar otorgar a cada proceso
el total de marcos que solicita, pero esto tendría como resultado
disminuir el grado de multiprogramación y, por tanto, reducir el uso
efectivo total del procesador.

Otra alternativa es la /asignación igualitaria/: se divide el total de
espacio en memoria física entre todos los procesos en ejecución, en
partes iguales. Esto es, volviendo a la computadora hipotética que se
presentó al inicio de esta sección, si hay cuatro procesos que
requieren ser ejecutados, de los 194 marcos disponibles, el sistema
asignará 48 (192 *kb*) a dos de los procesos y 49 (196 *kb*) a los otros
dos (es imposible asignar fracciones de marcos). De este modo, el
espacio será compartido por igual.

La asignación igualitaria resulta ser un esquema deficiente para casi
todas las distribuciones de procesos: bajo este esquema, si $P_1$ es
un gestor de bases de datos que puede estar empleando $2~048$ *kb* (512
páginas) de memoria virtual (a pesar de que el sistema tiene sólo 1 *mb*
de memoria física) y $P_2$ es un lector de texto que está empleando un
usuario, requiriendo apenas 112 *kb* (28 páginas), con lo cual incluso
dejaría algunos de sus marcos sin utilizar.

Un segundo esquema, que resuelve mejor esta situación, es la
/asignación proporcional/: dar a cada proceso una porción del espacio
de memoria física proporcional a su uso de memoria virtual.

De tal suerte que, si además de los procesos anteriores se tiene a $P_3$
empleando 560 *kb* (140 páginas) y a $P_4$ con 320 *kb* (80 páginas) de
memoria virtual, el uso total de memoria virtual sería de $V_T = 512 +
28 + 140 + 80 = 760$ páginas, esto es, el sistema tendría comprometido
mediante la memoria virtual un sobreuso cercano a 4:1 sobre la
memoria física.[fn:: Ya que de los $1~024$ *kb*, o 256 páginas, que tiene
el sistema descrito, descontando los 248 *kb*, o 62 páginas, que ocupa el
sistema operativo, quedan 194 páginas disponibles para los procesos.]

Cada proceso recibirá entonces $F_P = \frac{V_P}{V_T} \times m$, donde
$F_P$ indica el espacio de memoria física que el proceso recibirá,
$V_P$ la cantidad de memoria virtual que está empleando, y $m$ la
cantidad total de marcos de memoria disponibles. De este modo, $P_1$
recibirá 130 marcos, $P_2$ 7, $P_3$ 35 y $P_4$ 20, proporcionalmente a
su uso de memoria virtual.

Cabe apuntar que este mecanismo debe observar ciertos parámetros
mínimos: por un lado, si el mínimo de marcos definido para esta
arquitectura es de cuatro, por más que entrara en ejecución un proceso
de 32 *kb* (ocho páginas) o aumentara al doble el grado de
multiprocesamiento, ningún proceso debe tener asignado menos del
mínimo definido.

La asignación proporcional también debe cuidar no sobre-asignar
recursos a un proceso /obeso/: $P_1$ es ya mucho más grande que todos
los procesos del sistema. En caso de que esta creciera mucho más, por
ejemplo, si multiplicara por cuatro su uso de memoria virtual, esto
llevaría a que se /castigara/ desproporcionadamente a todos los demás
procesos del sistema.

Por otro lado, este esquema ignora por completo las prioridades que
hoy en día manejan todos los sistemas operativos; si se quisiera
considerar, podría incluirse como factor la prioridad, multiplicando
junto con $V_P$.

El esquema de asignación proporcional sufre, sin embargo, cuando
cambia el nivel de multiprogramación, esto es, cuando se inicia un
nuevo proceso o finaliza un proceso en ejecución, deben recalcularse
los espacios en memoria física asignados a cada uno de los procesos
restantes. Si finaliza un proceso, el problema es menor, pues sólo se
asignan los marcos y puede esperarse a que se vayan poblando por
paginación sobre demanda, pero si inicia uno nuevo, es necesario
reducir de golpe la asignación de todos los demás procesos hasta abrir
suficiente espacio para que quepa.

Por último, el esquema de la asignación proporcional también tiende a
desperdiciar recursos: si bien hay procesos que mantienen un patrón
estable de actividad a lo largo de su ejecución, muchos otros pueden
tener periodos de mucho menor requisitos. Por ejemplo, un proceso
servidor de documentos pasa la mayor parte de su tiempo simplemente
esperando solicitudes, y podría reducirse a un uso mínimo de memoria
física, sin embargo, al solicitársele un documento, se le deberían
poder asignar más marcos (para trabajar en una /ráfaga/) hasta que
termine con su tarea. En la sección \ref{MEM_mod_conjunto_activo} se
retomará este tema.

*** Ámbitos del algoritmo de reemplazo de páginas

Para atender los problemas no resueltos que se describieron en la
sección anterior, se puede discutir el ámbito en que operará el
algoritmo de reemplazo de páginas.

- Reemplazo local :: Mantener tan estable como sea posible el cálculo
     hecho por el esquema de asignación empleado. Esto significa que
     cuando se presente un fallo de página, las únicas páginas que se
     considerarán para su intercambio serán aquellas pertenecientes
     /al mismo proceso/ que el que causó el fallo.

     Un proceso tiene asignado su espacio de memoria física, y se
     mantendrá estable mientras el sistema operativo no tome alguna
     decisión por cambiarlo.

- Reemplazo global :: Los algoritmos de asignación determinan el
     espacio asignado a los procesos al ser inicializados, e influyen
     a los algoritmos de reemplazo (por ejemplo, dando mayor peso para
     ser elegidas como páginas víctima a aquellas que pertenezcan a un
     proceso que excede de su asignación en memoria física).

     Los algoritmos de reemplazo de páginas operan sobre el espacio
     completo de memoria, y la asignación física de cada proceso puede
     variar según el estado del sistema momento a momento.

- Reemplazo global con prioridad :: Es un esquema mixto, en el que un
     proceso puede /sobrepasar/ su límite siempre que le /robe/
     espacio en memoria física exclusivamente a procesos de prioridad
     inferior a él. Esto es consistente con el comportamiento de los
     algoritmos planificadores, que siempre dan preferencia a un
     proceso de mayor prioridad por sobre de uno de prioridad más
     baja.

El reemplazo local es más rígido y no permite mejorar el rendimiento
que tendría el sistema si aprovechara los periodos de inactividad
de algunos de los procesos. En contraposición, los esquemas basados en
reemplazo global pueden llevar a rendimiento inconsistente: dado que la
asignación de memoria física sale del control de cada proceso puede que la
misma sección de código presente tiempos de ejecución muy distintos si
porciones importantes de su memoria fueron paginadas a disco.
** Hiperpaginación
# <<MEM_hiperpaginacion>>

Es un fenómeno que se puede presentar por varias razones: cuando (bajo
un esquema de reemplazo local) un proceso tiene asignadas pocas
páginas para llevar a cabo su trabajo, y genera fallos de página con
tal frecuencia que le imposibilita realizar trabajo real. Bajo un
esquema de reemplazo global, cuando hay demasiados procesos en
ejecución en el sistema y los constantes fallos y reemplazos hacen
imposible a todos los procesos involucrados avanzar, también se
presenta hiperpaginación.[fn:: Una traducción literal del término
/thrashing/, empleado en inglés para designar a este fenómeno, resulta
más gráfica: /paliza/.]

Hay varios escenarios que pueden desencadenar la hiperpaginación, y
su efecto es tan claro e identificable que prácticamente cualquier
usuario de cómputo lo sabrá reconocer. A continuación se presentará un
escenario ejemplo en que las malas decisiones del sistema operativo
pueden conducirlo a este estado.

Suponga un sistema que está con una carga media normal, con un
esquema de reemplazo global de marcos. Se lanza un nuevo proceso, que
como parte de su inicialización requiere poblar diversas estructuras a
lo largo de su espacio de memoria virtual. Para hacerlo, lanza una
serie de fallos de página, a las que el sistema operativo responde
reemplazando a varios marcos pertenecientes a otros procesos.

Casualmente, a lo largo del periodo que toma esta inicialización
(que puede parecer una eternidad: el disco es entre miles y
millones de veces más lento que la memoria) algunos de estos procesos
solicitan los espacios de memoria que acaban de ser enviados a disco,
por lo cual lanzan nuevos fallos de página.

Cuando el sistema operativo detecta que la utilización del procesador decrece,
puede aprovechar la situación para lanzar procesos de mantenimiento. 
Se lanzan estos procesos, reduciendo aún más el espacio de memoria física 
disponible para cada uno de los procesos preexistentes.

Se ha formado ya toda una cola de solicitudes de paginación, algunas
veces contradictorias. El procesador tiene que comenzar a ejecutar
=NOOP= (esto es, no tiene trabajo que ejecutar), porque la mayor
parte del tiempo lo pasa en espera de un nuevo marco por parte del
disco duro. El sistema completo avanza cada vez más lento.

#+label: MEM_graf_hiperpaginacion
#+caption: Al aumentar demasiado el grado de multiprogramación, el uso del *cpu* cae abruptamente, entrando en hiperpaginación  (Silberschatz, Galvin y Gagné 2010: 349).
#+attr_html: height="400"
#+attr_latex: width=0.7\textwidth
[[./img/gnuplot/hiperpaginacion.png]]

Los síntomas de la hiperpaginación son muy claros, y no son difíciles
de detectar. ¿Qué estrategia puede emplear el sistema operativo una
vez que se da cuenta que se presentó esta situación?

Una salida sería reducir el nivel de multiprogramación —si la
paginación se presentó debido a que los requisitos de memoria de los
procesos actualmente en ejecución no pueden ser satisfechos con la
memoria física disponible, el sistema puede seleccionar uno (o más) de
los procesos y suspenderlos por completo hasta que el sistema vuelva a
un estado normal. Podría seleccionarse, por ejemplo, al proceso con
menor prioridad, al que esté causando más cantidad de fallos, o al que
esté ocupando más memoria.

*** Modelando el /conjunto activo/
# <<MEM_mod_conjunto_activo>>

Un pico en la cantidad de fallos de página no necesariamente
significa que se va a presentar una situación de hiperpaginación
—muchas veces indica que el proceso cambió su /atención/ de un
conjunto de páginas a otro, o dicho de otro modo, que cambió el
/conjunto activo/ del proceso, y resulta natural que, al cambiar el
conjunto activo, el proceso accese de golpe una serie de páginas que
no había referenciado en cierto tiempo.

#+label: MEM_conjunto_activo
#+caption: Los picos y valles en la cantidad de fallos de página de un proceso definen su /conjunto activo/.
#+attr_html: height="400"
#+attr_latex: width=0.7\textwidth
[[./img/gnuplot/conjunto_activo.png]]

El /conjunto activo/ es, pues, la aproximación más clara a la
/localidad de referencia/ de un proceso dado: el conjunto de páginas
sobre los que está iterando en un momento dado.

Idealmente, para evitar los problemas relacionados con la
hiperpaginación, el sistema debe asignar a cada proceso suficientes páginas
como para que mantenga en memoria física su conjunto activo —y si no
es posible hacerlo, el proceso es un buen candidato para ser
suspendido. Sin embargo, detectar con suficiente claridad como para
efectuar este diagnóstico /cuál/ es el conjunto activo es una
tarea muy compleja, que típicamente implica rastrear y verificar del
orden de los últimos miles a decenas de miles de accesos a memoria.

* Consideraciones de seguridad

Para una cobertura a mayor profundidad del material presentado en
esta sección, se sugiere estudiar los siguientes textos:

- Smashing The Stack For Fun And Profit \parencite{AlephOne1996}
- The Tao of Buffer Overflows \parencite{TaoBufferOverflow}

** Desbordamientos de buffer (/buffer overflows/)
# <<MEM_buffer_overflow>>

Una de las funciones principales de los sistemas operativos en la
que se ha insistido a lo largo del libro es la de implementar protección entre
los procesos pertenecientes a diferentes usuarios, o ejecutándose con
distinto nivel de privilegios. Y si bien el enfoque general que
se ha propuesto es el de analizar por separado subsistema por
subsistema, al hablar de administración de memoria es necesario
mencionar también las implicaciones de seguridad que del presente
tema se pueden desprender.

En las computadoras de arquitectura von Neumann, todo dato a ser
procesado (sean instrucciones o datos) debe pasar por la memoria, por
el /almacenamiento primario/. Sólo desde ahí puede el procesador
leer la información directamente.

A lo largo del presente capítulo se ha mencionado que la *mmu* incluye ya
desde el hardware el concepto de /permisos/, separando claramente las
regiones de memoria donde se ubica el código del programa (y son, por
tanto, ejecutables y de sólo lectura) de aquéllas donde se encuentran
los datos (de lectura y escritura). Esto, sin embargo, no los pone a
salvo de los /desbordamientos de buffer/ (/buffer overflows/),
errores de programación (típicamente, la falta de verificación de
límites) que pueden convertirse en vulnerabilidades.[fn:: Citando a
Theo de Raadt, autor principal del sistema operativo
#+latex: Open\textbf{bsd},
#+html: Open<b>bsd</b>,
todo error es una vulnerabilidad esperando a ser descubierta.]

*** La /pila de llamadas/ (stack)

Recordando lo mencionado en la sección \ref{MEM_espacio_en_memoria},
en que se presentó el espacio en memoria de un proceso, es conveniente
profundizar un poco más acerca de cómo está estructurada la 
/pila de llamadas/ (/stack/).

El /stack/ es el mecanismo que brinda un sentido local a la
representación del código estructurado. Está dividido en /marcos de
activación/ (sin
relación con el concepto de marcos empleado al hablar de memoria
virtual); durante el periodo en que es el marco /activo/ (esto es,
cuando no se ha transferido el control a ninguna otra función), está
delimitado por dos valores, almacenados en registros:

- Apuntador a la pila :: (/Stack pointer/, *sp*) Apunta al /final
     actual/ (dirección inferior) de la pila. En arquitecturas x86,
     emplea el registro =ESP=; cuando se pide al procesador que actúe sobre
     el /stack/ (con las operaciones =pushl= o =popl=), lo hace sobre
     este registro.
- Apuntador del marco :: (/Frame pointer/, *fp*, o /Base local/, *lb*)
     Apunta al /inicio/ del marco actual, o lo que es lo mismo, al
     final del marco anterior. En arquitecturas x86, emplea el
     registro =EBP=.

A cada función a la cual va entrando la ejecución del proceso, se va
creando un /marco de activación/ en el /stack/, que incluye:

- Los argumentos recibidos por la función.
- La dirección de retorno al código que la invocó.
- Las variables locales creadas en la función.

Con esto en mente, es posible  analizar la traducción
de una llamada a función en C a su equivalente en ensamblador, y en
segundo término ver el marco del /stack/ resultante:

#+begin_src c
void func(int a, int b, int c) {
   char buffer1[5];
   char buffer2[10];
}

void main() {
  func(1,2,3);
}
#+end_src

Y lo que el código resultante en ensamblador efectúa es:

1. El procesador /empuja/ (=pushl=) los tres argumentos al /stack/
   (=ESP=). La notación empleada (=$1=, =$2=, =$3=) indica que el
   número indicado se expresa de forma literal. Cada uno de estos tres
   valores restará 4 bytes (el tamaño de un valor entero en x86-32) a
   =ESP=.
2. En ensamblador, los nombres asignados a las variables y
   funciones no significan nada. La llamada =call= no es lo que
   se entendería como una llamada a función en un lenguaje de alto
   nivel —lo que hace el procesador es /empujar/ al /stack/ la dirección
   de la siguiente instrucción, y cargar a éste la dirección
   en el fuente donde está la etiqueta de la función (esto es, transferir la
   ejecución hacia allá).
3. Lo primero que hace la función al ser invocada es asegurarse de
   saber a dónde volver: /empuja/ al /stack/ el viejo apuntador al marco
   (=EBP=), y lo reemplaza (=movl=) por el actual. A esta ubicación se
   le llama =SFP= (/Saved Frame Pointer/, /apuntador al marco
   grabado/)
4. Por último, con =subl=, resta el espacio necesario para alojar las
   variables locales, =buffer1= y =buffer2=. Notarán que, si bien
   éstas son de 5 y 10 bytes, está recorriendo 20 bytes —esto
   porque, en la arquitectura x86-32, los accesos a memoria deben
   estar /alineados a 32 bits/.

#+begin_src [x86masm]Assembler
; main
        pushl $3
        pushl $2
        pushl $1
        call func

func:
        pushl %ebp
        movl %esp,%ebp
        subl $20,%esp
#+end_src

La figura \ref{MEM_stackframe} ilustra cómo queda la región inferior
del /stack/ (el espacio de trabajo de la función actual) una vez que
tuvieron lugar estos cuatro pasos.

#+attr_latex: width=0.8\textwidth
#+label: MEM_stackframe
#+caption: Marco del /stack/ con llamada a =func(1,2,3)= en x86-32.
[[./img/ditaa/stackframe.png]]

*** C y las funciones de manejo de cadenas

El lenguaje de programación C fue creado con el propósito de ser tan
simple como sea posible, manteniéndose tan cerca del hardware como se
pudiera, para que pudiera ser empleado como un lenguaje de
programación para un sistema operativo portable.  Y si bien en 1970
era visto como un lenguaje relativamente de alto nivel, hoy en día
puede ubicarse como el más bajo nivel en que programa la mayor parte
de los desarrolladores del mundo.

C no tiene soporte nativo para /cadenas/ de caracteres. El soporte es
provisto mediante /familias/ de funciones en la biblioteca estándar
del lenguaje, que están siempre disponibles en cualquier
implementación estándar de C. Las familias principales son =strcat=,
=strcpy=, =printf= y =gets=. Estas funciones trabajan con cadenas que
siguen la siguiente estructura:

- Son arreglos de 1 o más caracteres (=char=, 8 bits).
- /Deben/ terminar con el byte de terminación *nul* (=\0=).

El problema con estas funciones es que sólo algunas de las funciones
derivadas implementan verificaciones de límites, y algunas son incluso
capaces de crear cadenas ilegales (que no concluyan con el terminador
=\0=).

El problema aparece cuando el programador no tiene el cuidado
necesario al trabajar con datos de los cuales no tiene /certeza/. Esto
se demuestra con el siguiente código vulnerable:

#+begin_src c
#include <stdio.h>
int main(int argc, char **argv) {
        char buffer[256];
        if(argc > 1) strcpy(buffer, argv[1]);
        printf("Escribiste %s\n", buffer);
        return 0;
}
#+end_src

El problema con este código reside en el =strcpy(buffer, argv[1])=
—dado que el código es recibido del usuario, no se tiene la /certeza/
de que el argumento que recibe el programa por línea de comandos
(empleando =argv[1]=) quepa en el arreglo =buffer[256]=. Esto es, si
se ejecuta el programa ejemplo con una cadena de 120 caracteres:

#+begin_src sh
$ ./ejemplo1 `perl -e 'print "A" x 120'`
Escribiste: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAA
$
#+end_src


La ejecución resulta exitosa. Sin embargo, si se ejecuta el programa con un
parámetro demasiado largo para el arreglo:

#+begin_src sh
$ ./ejemplo1 `perl -e 'print "A" x 500'`
Escribiste: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
Segmentation fault
$
#+end_src

*** De una falla a un ataque

En el ejemplo recién presentado, parecería que el sistema /atrapó/ al
error exitosamente y detuvo la ejecución, pero no lo hizo: el
=Segmentation fault= no fue generado al sobreescribir el buffer ni al
intentar procesarlo, sino después de terminar de hacerlo: al llegar la
ejecución del código al =return 0=. En este punto, el /stack/ del
código ejemplo luce como lo presenta la figura \ref{MEM_overflow_500}.

#+attr_latex: width=0.8\textwidth
#+label: MEM_overflow_500
#+caption: Estado de la memoria después del =strcpy()=.
[[./img/ditaa/overflow_500.png]]

Para volver de una función a quien la invocó, incluso si dicha
función es =main()=, lo que hace =return= es restaurar el viejo =SFP=
y hacer que el apuntador a siguiente dirección /salte/ a la
dirección que tiene en =RET=. Sin embargo, como se observa en el
esquema, =RET= fue sobreescrito por la dirección =0x41414141=
(=AAAA=). Dado que esa dirección no forma parte del espacio del
proceso actual, se lanza una excepción por violación de segmento, y
el proceso es terminado.

Ahora, lo expuesto anteriormente implica que el código /es demostrado
vulnerable/, pero no se ha /explotado/ aún. El siguiente paso es,
conociendo el acomodo exacto de la memoria, sobreescribir únicamente
lo necesario para alterar el flujo del programa, esto es,
sobreescribir =RET= con una dirección válida. Para esto, es necesario
conocer la longitud desde el inicio del buffer hasta donde terminan
=RET= y =SFP=, en este caso particular, 264 bytes (256 del buffer más
cuatro de =RET= más cuatro de =SFP=).

Citando al texto de Enrique Sánchez,

#+BEGIN_QUOTE
¿Por qué ocurre un desbordamiento de /stack/? Imagina un vaso y una
botella de cerveza. ¿Qué ocurre si sirves la botella completa en el
vaso? Se va a derramar. Imagina que tu variable es el vaso, y la
entrada del usuario es la cerveza. Puede ocurrir que el usuario sirva
tanto líquido como el que cabe en el vaso, pero puede también seguir
sirviendo hasta que se derrame. La cerveza se derramaría en todas
direcciones, pero la memoria no crece de esa manera, es sólo un
arreglo bidimensional, y sólo crece en una dirección.

Ahora, ¿qué más pasa cuando desbordas un contenedor? El líquido
sobrante va a mojar la botana, los papeles, la mesa, etc. En el caso
de los papeles, destruirá cualquier cosa que hubieras apuntado (como
el teléfono que acabas de anotar de esa linda chica). Cuando tu
variable se desborde, ¿qué va a sobrescribir? Al =EBP=, al =EIP=, y lo
que les siga, dependiendo de la función, y si es la última función,
las variables de ambiente. Puede que el programa aborte y tu shell
resulte inutilizado a causa de las variables sobreescritas.
#+END_QUOTE

Hay dos técnicas principales: /saltar/ a un punto determinado del
programa, y /saltar/ hacia dentro del /stack/.

Un ejemplo de la primera técnica se muestra a continuación. Si el 
atacante está intentando burlar la siguiente validación simple de
nombre de usuario y contraseña,

#+begin_src c
if (valid_user(usr, pass)) {
  /* (...) */
} else {
  printf("Error!\n");
  exit 1;
}
#+end_src

Y detecta que =valid_user()= es susceptible a un desbordamiento, le
bastaría con incrementar en cuatro la dirección de retorno. La conversión
de este =if= a ensamblador es, primero, saltar hacia la etiqueta
=valid_user=, e ir (empleando al valor que ésta regrese en =%EBX=) a
la siguiente instrucción, o saltar a la etiqueta =FAIL=. Esto puede
hacerse con la instrucción =BNE $0, %EBX, FAIL= (/Branch if Not
Equal/, /saltar si no es igual/, que recibe como argumentos dos
valores a ser comparados, en este caso el registro =%EBX= y el número
0, y la etiqueta destino, =FAIL=). Cambiar la
dirección destino significa burlar la verificación.

Por otro lado, el atacante podría usar la segunda técnica
para lograr que el sistema haga algo más complejo —por ejemplo, que ejecute 
código arbitrario que él proporcione. Para esto, el ataque más frecuente 
es saltar /hacia adentro del stack/.

#+attr_latex: width=0.8\textwidth
#+label: MEM_overflow_jump
#+caption: Ejecutando el código arbitrario inyectado al buffer.
[[./img/ditaa/overflow_jump.png]]

Para hacerlo, si en vez de proporcionar simplemente una cadena
suficientemente grande para sobrepasar el buffer se /inyecta/ una
cadena con código ejecutable válido, y sobreescribiera la dirección de
retorno con la dirección de su código /dentro del buffer/, tendría 256
bytes de espacio para especificar código arbitrario. Este código
típicamente se llama /shellcode/, pues se emplea para obtener un
/shell/ (un intérprete de comandos) que ejecuta con los privilegios
del proceso explotado. Este escenario se ilustra en la figura
\ref{MEM_overflow_jump}.

*** Mecanismos de mitigación

Claro está, el mundo no se queda quieto. Una vez que estos mecanismos
de ataque se dieron a conocer, comenzó un fuerte trabajo para crear
mecanismos de mitigación de daños.

La principal y más importante medida es crear una cultura de
programadores conscientes y prácticas seguras. Esto cruza
necesariamente el no emplear funciones que no hagan verificación de
límites. La desventaja de esto es que hace falta cambiar al /factor
humano/, lo cual resulta prácticamente imposible de lograr con
suficiente profundidad.[fn:: El ejemplo más claro de este problema es la función
/gets/, la cual sigue siendo enseñada y usada en los cursos básicos de
programación en C.] Muchos desarrolladores esgrimen argumentos en
contra de estas prácticas, como la pérdida de rendimiento que estas
funciones requieren, y muchos otros sencillamente nunca se dieron por
enterados de la necesidad de programar correctamente.

Por esto, se han ido creando diversos mecanismos automatizados de
protección ante los desbordamientos de buffer. Ninguno de estos
mecanismos es /perfecto/, pero sí ayudan a reducir los riesgos ante
los atacantes menos persistentes o habilidosos.

*** Secciones de datos no ejecutables

En secciones anteriores se describió la protección que puede imponer
la *mmu* por regiones, evitando la modificación de código ejecutable.

En la arquitectura x86, dominante en el mercado de computadoras
personales desde hace muchos años, esta característica existía en
varios procesadores basados en el modelo de segmentación de memoria,
pero desapareció al cambiarse el modelo predominante por uno de
memoria plana paginada, y fue hasta alrededor del 2001 en que fue
introducida de vuelta, bajo los nombres /bit NX/ (/Never eXecute/,
nunca ejecutar) o /bit XD/ (/eXecute Disable/, deshabilitar
ejecución), como una característica particular de las extensiones *pae*.

Empleando este mecanismo, la *mmu* puede evitar la ejecución de código
en el área de /stack/, lo cual anula la posibilidad de /saltar al
stack/. Esta protección desafortunadamente no es muy efectiva: una vez
que tiene acceso a un buffer vulnerable, el atacante puede /saltar a
libc/, esto es, por ejemplo, proporcionar como parámetro el nombre de
un programa a ejecutar, e indicar como retorno la dirección de la
función =system= o =execve= de la =libc=.

Las secciones de datos no ejecutables son, pues, un obstáculo ante un
atacante, aunque no representan una dificultad mucho mayor.

*** Aleatorización del espacio de direcciones

Otra técnica es que, en tiempo de carga y a cada ejecución, el proceso
reciba diferentes direcciones base para sus diferentes áreas. Esto
hace más difícil para el atacante poder indicar a qué dirección
destino se debe saltar.

Un atacante puede emplear varias técnicas para ayudarse a /adivinar/
detalles acerca del acomodo en memoria de un proceso, y, con un
buffer suficientemente grande, es común ver /cadenas de NOP/, esto
es, una extensión grande de operaciones nulas, seguidas del
/shellcode/, para aumentar las probabilidades de que el control se
transfiera a un punto útil.

*** Empleo de /canarios/

Se llama /canario/ a un valor aleatorio de protección,[fn:: Este uso
proviene de la costumbre antigua de los mineros de tener un canario en
una jaula en las minas. Como el canario es muy sensible ante la falta de
oxígeno, si el canario moría servía como indicador a los mineros de
que debían abandonar la mina de inmediato, antes de correr la misma
suerte.] insertado entre los buffers y la dirección de retorno, que
es verificado antes de regresar de una función. Si se presentó un
desbordamiento de buffer, el valor del /canario/ será reemplazado por
basura, y el sistema podrá detener la ejecución del proceso
comprometido antes de que brinde privilegios elevados al atacante. La
figura \ref{MEM_overflow_canary} ilustra este mecanismo.

#+attr_latex: width=0.9\textwidth
#+label: MEM_overflow_canary
#+caption: Marco de /stack/ con un /canario/ aleatorio protector de 12 bytes (=qR'z2a&5f50s=): si este es sobreescrito por un buffer desbordado, se detendrá la ejecución del programa.
[[./img/ditaa/overflow_canary.png]]

Un atacante tiene dos mecanismos ante un sistema que requiere del
canario: uno es el atacar no directamente a la función en cuestión,
sino al /manejador de señales/ que es notificado de la anomalía, y
otro es, ya que se tiene acceso a la memoria del proceso, /averiguar
el valor del canario/. Esto requiere ataques bastante más sofisticados
que los vistos en esta sección, pero definitivamente ya no fuera del
alcance de los atacantes.

** Ligado estático y dinámico de bibliotecas

Las /bibliotecas de código/ (o simplemente /bibliotecas/) implementan el
código de una serie de funcionalidades generales, que
pueden ser usadas en diferentes programas y contextos. Un ejemplo clásico sería la
biblioteca estándar de C, la cual ofrece funciones básicas
de entrada/salida, manejo de cadenas, entre otras.

A medida que el software crece en complejidad, los programadores
recurren a la /reutilización de código/ para aprovechar la
implementación de la funcionalidad que ofrecen las distintas
bibliotecas.  De esta forma, evitan "reinventar la rueda", y se
concentran en la funcionalidad específica del software que están
construyendo.

El concepto de /ligado/ se refiere al proceso mediante el cual, se
toma el /código objeto/ de un programa junto con el código de las
bibliotecas que éste usa para crear un archivo ejecutable. De forma
general hay dos tipos de ligado, que se explican a continuación.

El /ligado estático/ consiste en tomar el código de una biblioteca e
integrarlo al código del programa para generar el archivo
ejecutable. Lo anterior implica que /cada programa/ tiene su propia copia del
código de la biblioteca, lo cual puede causar un desperdicio de
memoria y disco si hay muchos programas que usan la misma versión de ésta.

Por su parte, en el /ligado dinámico/ el código de las bibliotecas no
se copia dentro de la imagen ejecutable del programa, pero requiere
establecer algún mecanismo para informar que el programa necesita un
código externo. Esto se puede implementar de diferentes formas. Por
ejemplo, se puede incluir un fragmento de código dentro del programa
que usa la biblioteca denominado /stub/, el cual en tiempo de
ejecución solicita que se cargue la biblioteca requerida. Otra
estrategia que se puede utilizar consiste en incluir algunas
indicaciones que le permiten al sistema operativo, en el momento de
crear el proceso, ubicar las bibliotecas que este requerirá para su
ejecución. En cualquier caso, el ligado dinámico busca que las
bibliotecas sólo sean cargadas cuando se las requiera.

La figura \ref{MEM_tipo_resol_direcc}
(p. \pageref{MEM_tipo_resol_direcc}) ilustra el momento en que
ocurre cada uno de estos ligados:
el ligado estático es realizado por el /editor de ligado/, uniendo en
un solo /módulo cargable/ al programa compilado (/módulo objeto/) con
las bibliotecas (/otros objetos/); el ligado dinámico es realizado
parcialmente en tiempo de carga (para las /bibliotecas del sistema/) y
parcialmente en tiempo de ejecución (para las /bibliotecas de carga
dinámica/).[fn:: Refiérase al libro /Linkers and Loaders/ (ligadores y
cargadores) de John R. Levine (1999) para mayores detalles respecto a
este proceso.]

*** Las bibliotecas y la seguridad

El ligado dinámico puede traer consigo una serie de problemas, entre
los cuales se destacan el manejo de versiones de las bibliotecas y
potenciales vulnerabilidades.  El primer problema es conocido, en
ambientes Windows, como el /infierno de las DLL/. éste se puede causar
de muchas formas. Por ejemplo, si al instalar un nuevo programa, se
instala también una versión incompatible de una biblioteca que es
usada por otros programas.  Esto causa que los demás programas no se
puedan ejecutar, y lo que es más, hace que la depuración del fallo
sea muy difícil. Por otro lado, si no se tienen los controles
suficientes, al desinstalar un programa se puede borrar una biblioteca
compartida, lo cual puede llevar a que otros programas dejen de
funcionar.

El /infiero de las DLL/ puede ser prevenido mediante estrategias como
el /versionamiento/ de las biblioteca de ligado dinámico (esto es,
hacer que cada componente de las bibliotecas lleve la versión que
implementa o /nivel de compatibilidad/ que implementa),[fn:: Este
nivel de compatibilidad incluye no sólo a la /interfaz de aplicación
al programador/ (*api*, definida en las secciones \ref{HW_SYSCALLS} y
\ref{HW_syscall_arch_api}), sino también la /interfaz de aplicación
binaria/ (*abi*), esto es, no sólo la información del nombre de las
funciones que expone y los tipos de argumentos que reciben, sino
también la ubicación en memoria de su definición en un archivo ya
compilado.] y mediante el uso de scripts de instalación o /gestores de
dependencias/ que verifican si en el sistema hay una versión
compatible de la biblioteca. Teniendo esta información, la biblioteca
en cuestión se instalará únicamente en caso necesario.

El ligado dinámico puede presentar problemas o vulnerabilidades debido
a que el programa usa un código proporcionado por terceros, y /confía/
en que la biblioteca funciona tal como se espera sin incluir código
malicioso. Por tal razón, desde el punto de vista teórico bastaría que
un atacante instale su propia versión de una biblioteca para que pueda
tener el control de los programas que la usan e incluso del mismo
sistema operativo.[fn:: Esto se puede lograr, por ejemplo, alterando
la configuración del entorno en la cual el sistema busca las
bibliotecas.] En el caso de bibliotecas ligadas /estáticamente/, dado
que estas forman ya parte del programa, un atacante tendría que
modificar al archivo objeto mismo del programa para alterar las
bibliotecas.

Así las cosas, más allá de la economía de espacio en memoria, ¿cómo se
explica que sea tanto más popular el ligado dinámico en los sistemas
operativos modernos?

Parte muy importante de la respuesta es la /mantenibilidad/: si es
encontrado un fallo en una biblioteca de carga dinámica, basta con que
los desarrolladores lo corrijan una vez (cuidando, claro, de mantener
la compatibildad binaria) y reemplazar a dicha biblioteca en disco
/una sola vez/. Todos los programas que liguen dinámicamente con esta
biblioteca tendrán disponible de inmediato la versión actualizada. En
mucho sistemas operativos, el /gestor de paquetes/ puede detectar
cuáles procesos en ejecución emplean a determinada biblioteca
dinámica, y reiniciarlos de forma transparente al administrador.

En contraste, de estar el fallo en una biblioteca de ligado estático,
el código afectado estaría incluido como parte de /cada uno de los
programas/ ligados con ella. Como consecuencia, para corregir este
defecto, cada uno de los programas afectados tendría que ser
recompilado (o, por lo menos, /religado/) antes de poderse beneficiar
de las correcciones.

Y si bien este proceso resulta manual y tedioso para un administrador
de sistemas con acceso a las fuentes de los programas que tiene
instalados, resulta mucho más oneroso aún para quienes emplean
software /propietario/ (en la sección \ref{SL_propietario} se aborda
con mayor detenimiento lo que significa el software propietario en
contraposición al software libre).

Cabe mencionar que el comportamiento del sistema ante la actualización
de una biblioteca descrita ilustra una de las diferencias semánticas
entre sistemas Windows y sistemas Unix que serán abordadas en el
capítulo \ref{DIR}: mientras un sistema Unix permite la eliminación de
un archivo /que está siendo utilizado/, Windows no la permite. Esto
explica por qué las actualizaciones de bibliotecas en sistemas Windows
se aplican /durante el proceso de apagado/: mientras haya procesos que
tienen abierta una biblioteca, ésta no puede ser reemplazada. Caso
contrario en sistemas Unix, en que el archivo puede ser sustituido,
pero mientras no sean reiniciados los procesos en cuestión, éstos
seguirán ejecutando la versión de la biblioteca con el error.


# * Consideraciones de rendimiento
# #+begin_src c
#  int datos[8000][8000];
#  
#  /* Versión correcta: El ciclo externo va sobre el */
#  /* índice externo, el ciclo interno va sobre el índice */
#  /* interno. El arreglo en memoria sera recorrido */
# /* secuencialente. */
#  for (i=0; i<8000; i++) {
#    for (j=0; j<8000; j++) {
#      datos[i][j] += 1;
#    }
#   }
#  
#  /* Si el ciclo externo es empleado para el índice interno */
#  /* y viceversa, el recorrido en memoria sera de "puntitos" */
#  /* distribuidos por todo el espacio que ocupa datos[], */
#  /* reemplazando a los contenidos del TLB una y otra vez, y */
#  /* con un fuerte impacto al rendimiento. */
#  for (i=0; i<8000; i++) {
#    for (j=0; j<8000; j++) {
#      datos[j][i] += 1;
#    }
#   }
# #+end_src

* Ejercicios

** Preguntas de autoevaluación
1. Diagrame el acomodo del espacio en memoria de un proceso. ¿Qué
   diferencias principales y qué similitudes tienen la /sección de
   datos/ con el /espacio de libres/ (/heap/), y entre el /espacio de
   libres/ con la /pila/ (/stack/)?

2. Un proceso en un sistema con arquitectura de memoria basada en la
   /segmentación/ tiene la siguiente tabla de segmentos:

    | Segmento | Inicio  | Tamaño | Permisos |
    |----------+---------+--------+----------|
    |        0 | 240     |    600 | *rx*     |
    |        1 | $2~300$ |     16 | *r*      |
    |        2 | 90      |    100 | *rw*     |
    |        3 | $1~320$ |    950 | *rw*     |
    |        4 | -       |     96 | *rx*     |

   Para cada una de las siguientes solicitudes, indique qué dirección
   física correspondería, y –de ser el caso– qué excepción se genera.

   1. Lectura, 0-430
   2. Escritura, 0-150
   3. Lectura, 1-15
   4. Escritura, 2-130
   5. Ejecución, 4-25

3. El buffer de traducción adelantada (*tlb*) de un sistema en
   particular presenta una efectividad de 95%. Obtener un valor del
   *tlb* toma 10ns. La memoria principal tarda 120ns en recuperar un
   valor. ¿Cuál es el tiempo promedio para completar una operación a
   memoria?

4. Con la siguiente cadena de referencia, y empleando cuatro marcos de
   memoria física, desarrolle la asignación bajo los esquemas *fifo*,
   *opt* y *lru*:
   #+BEGIN_QUOTE
   1, 3, 2, 1, 5, 3, 4, 1, 5, 2, 6, 7, 5, 7, 2, 5, 3, 5, 3, 1
   #+END_QUOTE
   Asumiendo que cada fallo de página toma ocho milisegundos en ser
   atendido, ¿qué diferencia en rendimiento puede observarse?

5. Suponga un sistema paginado con un rango de direcciones de 4 *gb*
   ($4~294~967~296$ direcciones).
   - ¿Cuántas páginas tendrá el sistema si se utilizan páginas de
     $4~096$ bytes?
   - ¿Qué tamaño (en bits) tendrá una entrada de la tabla de
     traducción? Suponga que sólo se guarda el número de marco
     físico.
   - ¿Qué tamaño tendrá la tabla de paginación si se desea cubrir todo el rango?
   - Suponga que el tamaño de la tabla de paginación fuera demasiado
     grande. Proponga dos soluciones explicando ventajas y desventajas
     de cada una.

6. Explique la relación que hay entre direcciones lógicas y
   direcciones físicas. Indique cómo se realiza la traducción en los
   siguientes casos:
   - Una computadora con *tlb* y tabla de paginación de un nivel, con la
     entrada cargada en la *tlb*.
   - Una computadora con *tlb* y tabla de paginación de un nivel y sin
     la entrada cargada.
   - Una computadora sin *tlb* y con tabla de paginación de dos niveles.
   - Una computadora sin paginación ni *tlb* pero con segmentación.

7. Un equipo presenta rendimiento muy deficiente. Ante un análisis de
   utilización a lo largo de un día, se encuentra que el promedio de
   uso de *cpu* está a 20% y el uso de la interfaz al disco duro que
   aloja a la memoria virtual a 95%. ¿En qué condición está el sistema?

   Elija de la siguiente lista las dos respuestas que /mayor
   efectividad/ tendrían para mejorar el rendimiento del
   sistema. Fundamente con una breve explicación.

   1. Reemplazar al *cpu* por uno 20% más rápido (pasar de 1 GHz a 1.2
      GHz).
   2. Destinar un área del disco 25% más grande para la memoria
      virtual (pasar de 4 a 5 *gb*).
   3. Aumentar el grado de multiprogramación en 10% (aumentar de 20 a
      22 los procesos en la cola de ejecución).
   4. Reducir el grado de multiprogramación en 10% (reducir de 20 a
      18 los procesos en la cola de ejecución).
   5. Instalar un 25% más de memoria principal (pasar de 2 a 2.5 *gb*).
   6. Instalar un disco duro un 33% más rápido (cambiar un disco de
      $5~400$ RPM por uno de $7~200$ RPM).

8. Describa en qué consiste un ataque de /desbordamiento de pila/
   (/stack overflow/), y un mecanismo de protección del sistema para
   contrarrestarlos. ¿Puede sugerir una manera en que un atacante
   podría burlar dicha protección?


** Temas de investigación sugeridos

- Esquemas de asignación de memoria en una realidad *numa* ::

  La realidad que se expuso en el capítulo respecto al
  multiprocesamiento simétrico como fuertemente dominante en relación
  a los sistemas *numa* se mantiene cierta... Pero va cambiando
  rápidamente, y los sistemas *numa* son cada vez más comunes.

  Claro está, la popularización de los sistemas *numa* tiene un alto
  efecto en cómo se manejan los esquemas de administración de
  memoria. Al entrar en juego la /afinidad/ de cada proceso a un *cpu*
  dado, la administración de la memoria y el planificador
  (despachador) de procesos quedan fuertemente interrelacionados.

  En el número de septiembre del 2013 de la revista "Communications of
  the *acm*" aparece un artículo corto, conciso y bastante
  interesante: "An overview of non-uniform memory access"
  \parencite{Lameter2013}. Sugerimos emplearlo como punto de partida.

- Cargado y ligado de programas ::

  Este tema ocupa un punto intermedio entre el estudio de los sistemas
  operativos y el de los compiladores. Para los lectores del presente
  libro, comprender este punto común puede resultar de interés.

  Al convertir un programa escrito en determinado lenguaje de
  programación por un humano en código ejecutable, no sólo hay que
  convertir las instrucciones y proveer las abstracciones mínimas,
  también es necesario que preparar al código para poder ser
  /reubicado/ en la memoria.

  El proceso de /cargado/ se refiere a cómo la imagen binaria del
  programa que está grabada en un medio de almacenamiento es
  modificada al llevarse a memoria para adecuarse al espacio que le
  asigna el sistema operativo; el proceso de /ligado/ es cómo se
  maneja su integración con las bibliotecas compartidas.

  En este proceso entran temas como la gestión de /memoria
  compartida/, las /indicaciones al compilador/ para generar /código
  independiente de su posición/, etcétera.

- Mecanismos para mantener la coherencia en caché ::

  En un entorno multiprocesador, el acceso concurrente a las mismas
  posiciones de memoria se vuelve muy complicado, por decir lo
  menos. Los distintos niveles de memoria caché tienen que sincronizar
  su operación (a altísima velocidad) y permitir que avancen los
  procesos compartiendo datos.

  Hay varios mecanismos para mantener la coherencia. Algunas líneas
  para comenzar la investigación pueden ser, para computadoras
  razonablemente pequeñas, los mecanismos /fisgones/ (/snoopy/) de
  invalidación y de actualización, y para equipos más grandes, los
  mecanismos por hardware de /directorio/; los protocolos /fisgones/
  son también conocidos por sus siglas, o por las universidades donde
  fueron diseñados: *msi*, *mesi* (Illinois), *mosi* (Berkeley),
  *moesi* y *mesif*.

- Relación con sistemas operativos en uso ::

  Este capítulo es probablemente el que mejor permite apreciar de forma
  directa la relación entre los temas cubiertos y su manejo en un
  sistema operativo actual. Una manera sencilla de comprender el
  efecto de los diversos parámetros del subsistema de memoria virtual
  es la empírica: ¿Cómo reacciona un sistema operativo de propósito
  general ante cambios en los distintos valores y umbrales; cuáles
  serían las condiciones límite; qué comportamientos esperaríamos
  llevando al sistema a estos extremos, y cuáles encontramos en la
  realidad; cómo es la interfaz al administrador del sistema?

  Prácticamente todos los sistemas operativos que se emplean hoy
  permiten ajustar estos valores, y la comparación entre sistemas
  operativos distintos seguramente resultará también interesante al
  lector; en el caso de Linux, se sugiere revisar la documentación de
  los parámetros de /sysctl/ relativos a la memoria virtual. Éstos
  están documentados en \parencite{LinuxProcSysVm}.

- Otras categorías de vulnerabilidad ::

  Este capítulo presentó el /desbordamiento de pila/, una de las
  vulnerabilidades clásicas y que más se siguen explotando al día de
  hoy. Se presentaron algunos mecanismos de mitigación, pero la
  conclusión es lapidaria: la correcta protección de límites es de
  responsabilidad exclusiva e ineludible del desarrollador.

  Sin embargo, el desbordamiento de pila no es la única
  vulnerabilidad. Hay varias vulnerabilidades relacionadas con el
  acomodo o el tamaño específico de los datos en la memoria. Dos
  ejemplos:

  - En agosto del 2013, se descubrió lo que se popularizó como una
    /cadena Unicode de la muerte/: un conjunto de caracteres que causa
    que cualquier programa que intente desplegarlos en pantalla en la
    línea de productos Apple se /caiga/. El artículo publicado por The
    Register \parencite{Williams2013} relata el proceso que llevó a
    averiguar, a partir del /reporte de falla/ generado por el sistema
    y analizando el contenido de la memoria que éste reporta, cómo se
    puede encontrar un /desbordamiento de entero/.

  - En abril de 2014 se dio a conocer un fallo en la biblioteca
    criptográfica
    #+latex: Open\textbf{ssl},
    #+html: Open<b>ssl</b>,
    particularmente en la rutina que implementa el /Heartbeat/. Dada
    la gran base instalada de usuarios de
    #+latex: Open\textbf{ssl},
    #+html: Open<b>ssl</b>,
    y lo sensible de la información que expone al atacante, desde su
    descubrimiento se dio por hecho que ésta es una de las
    vulnerabilidades con mayor efecto en escala mundial. Dado el
    renombre que implica descubrir una vulnerabilidad de este tamaño,
    los descubridores de este fallo montaron el sitio
    http://heartbleed.com/ donde hay todo tipo de ligas describiendo
    esta problemática; otro artículo,también publicado por The
    Register \parencite{Williams2014}, explica la problemática
    explotada muy claramente.

** Lecturas relacionadas
- \fullcite{CorbetGrumpy64}.  Experiencia del editor de Linux Weekly
  News al migrar a una arquitectura de 64 bits en 2004. Lo más
  interesante del artículo son los comentarios, ilustran buena parte
  de los pros y contras de una migración de 32 a 64 bits.
- \fullcite{Campbell2013}. Presenta un ejemplo de uso de la
  herramienta /Valgrind/, para encontrar problemas en la asignación,
  uso y liberación de memoria en un programa en C.
- \fullcite{Males2014} presenta ejemplos del uso de pmap en diferentes
  sistemas Unix.
- \fullcite{NetBSDpmap}. Más allá de simplemente mostrar la operación de
  una herramienta del sistema en Unix, esta página de manual ilustra
  claramente la estructura de la organización de la memoria.
- \fullcite{Levine1999}
- \fullcite{Belady1969}
- \fullcite{Gorman2004}. Libro de libre descarga y
  redistribución. Aborda a profundidad los mecanismos y algoritmos
  relativos a la memoria empleados por el sistema operativo
  Linux. Entra en detalles técnicos a profundidad, presentándolos poco
  a poco, por lo que no resulta demasiado complejo de leer. El primer
  tercio del libro describe los mecanismos, y los dos tercios
  restantes siguen el código comentado que los implementa.
- \fullcite{Swanson2003}
- \fullcite{TaoBufferOverflow}
- \fullcite{AlephOne1996},
- \fullcite{Kingcopes2013} Explica cómo puede burlarse la protección
  basada en aleatorización de direcciones (*alsr*) en Windows 7 y 8,
  logrando una dirección predecible de memoria hacia la cual saltar.
- \fullcite{Lameter2013}
# Liga muerta :( Como sea, me gustaría adaptar lo que relata para
# mencionarlo con memoria virtual
#
# - /Less swap/ (http://ghostbar.ath.cx/2013/01/27/less-swap/), Jose
#   Luis Rivas (2013). Entrada  Controlando el "swappiness" de Linux vía
#   =vm.swappiness=
# My old X61 has 2G in *ram* and 3G in Swap. It seems my using-habit for
# Google Chrome is heavy (more than 20 tabs open most of the time) so
# my swap is normally being used above the 1.8G-mark. That means, my
# disk is constantly being written and read, and since this is the
# original hard-drive scare comes to my mind: will I blow-up my disk
# if I continue doing this? and the heat this disk-use's generating
# was absurd.
#
# That's not good, at all, I prefer having cache used than swap; in
# Venezuela is not that easy to get a good sale on hard-drives (you
# know, is hard to get dollars here, since is illegal to buy them
# unless the govt gives you permission).
#
# So the first thing that comes to my mind is sysctl. If I wanted to
# configure swap it would be with a kernel parameter. So running sysctl
# -a | grep swap throws me vm.swappiness = 60, and a little google
# search tells me more about it.
#
# If the swappiness is close to 0 then it will use the swap only if it
# is really necessary, if it is close to 100 then it will use the swap
# aggressively. Mine was at 60, and it was really aggressive.
#
# The next steps for me was doing sudo echo 'vm.swappiness = 1' >>
# /etc/sysctl.conf && sudo sysctl -p.
#
# Now my system is really smooth and using Chrome + Terminal + Tmux +
# Rhythmbox is acceptable.
